Category: AI & ML foundations
Topic: AI Security

The large-scale popularization and advancement of AI require strong security assurance. AI security model focuses on two types of AI attacks and defenses. First, affecting the correctness of AI decisions: An attacker can sabotage or control an AI system or intentionally change the input so that the system makes a decision desired by the attacker. Second, attackers may steal the confidential data used to train an AI system or extract the AI model. With these problems in mind, AI system security should be dealt with from three aspects: AI attack mitigation, AI model security, and AI architecture security. In addition, the transparency and explainability of AI are the foundation of security. An AI system that is not transparent or explainable cannot undertake critical tasks involving personal safety and public safety.

In order to tackle the new AI security challenges, three layers of defense for deploying AI systems are recommended: 
1. Attack mitigation: Design defense mechanisms for known attacks. 
2. Model security: Enhance model robustness by various mechanisms such as model verification.
3. Architecture security: Build a secure architecture with multiple security mechanisms to ensure business security.

AI has great potential to build a better, smarter world, but at the same time faces severe security risks. Due to the lack of security consideration at the early development of AI algorithms, attackers are able to manipulate the inference results in ways that lead to misjudgment. In critical domains such as healthcare, transportation, and surveillance, security risks can be devastating. Successful attacks on AI systems can result in property loss or endanger personal safety. AI security risks exist not only in theoretical analyses but also in AI deployments. For instance, attackers can craft files to bypass AI-based detection tools or add noise to smart home voice control command to invoke malicious applications. Attackers can also tamper with data returned by a terminal or deliberately engage in malicious dialogs with a chat robot to cause a prediction error in the backend AI system. It is even possible to apply small stickers on traffic signs or vehicles that cause false inferences by autonomous vehicles.


To mitigate these AI security risks, AI system design must overcome five security challenges: 
1.	Software and hardware security: The code of applications, models, platforms, and chips may have vulnerabilities or backdoors that attackers can exploit. Further, attackers may implant backdoors in models to launch advanced attacks. Due to the inexplainability of AI models, the backdoors are difficult to discover. 
2.	Data integrity: Attackers can inject malicious data in the training stage to affect the inference capability of AI models or add a small perturbation to input samples in the inference stage to change the inference result. 
3.	Model confidentiality: Service providers generally want to provide only query services without exposing the training models. However, an attacker may create a clone model through a number of queries. 
4.	Model robustness: Training samples typically do not cover all possible corner cases, resulting in the insufficiency of robustness. Therefore the model may fail to provide correct inference on adversarial examples. 
5.	Data privacy: For scenarios in which users provide training data, attackers can repeatedly query a trained model to obtain users’ private information.

Typical AI Security Attacks
1.	Evasion Attack:
In an evasion attack, an attacker modifies input data so that the AI model cannot correctly identify the input. 
Adversarial examples: Studies show that deep learning systems can be easily affected by well-crafted input samples, which are called adversarial examples. They are usually obtained by adding small perturbations into the original legitimate samples. These changes are not noticeable to human eyes but greatly affect the output of deep learning models.
Transferability and black-box attacks: To generate adversarial examples, attackers need to obtain AI model parameters, but these parameters are difficult to obtain in some scenarios. An adversarial example generated against a model can also deceive another model as long as the training data of the two models are the same. Attackers can exploit this transferability to launch black-box attacks without knowing AI model parameters. To do that, an attacker queries a model multiple times, uses the query results to train a "substitute model," and finally uses the substitute model to generate adversarial examples, which can be used to deceive the original model.
2.	Poisoning:
AI systems are usually retrained using new data collected after deployment to adapt to changes in input distribution. For example, an Intrusion Detection System (IDS) continuously collects samples on a network and retrains models to detect new attacks. In a poisoning attack, the attacker may inject carefully crafted samples to contaminate the training data in a way that eventually impairs the normal functions of the AI system — for example, escaping AI security detection. Deep learning requires a large number of training samples, so it is difficult to guarantee the quality of samples. Mixing a small number of adversarial examples with training samples can significantly affect the accuracy of AI models. For example, by adding 8 percent of malicious training data, attackers can cause a 75-percent change of the dosages suggested by models for half of the patients. There are three kinds of poisoning attacks: optimal gradient attack, global optimum attack, and statistical optimization attack.
3.	Backdoor:
Like traditional programs, AI models can be embedded with backdoors. Only the person who makes backdoors knows how to trigger them; other people do not know of their existence nor can trigger them. Unlike traditional programs, a neural network model only consists of a set of parameters, without source code. Therefore, backdoors in AI models are harder to detect than in traditional programs. These backdoors are typically implanted by adding some specific neuron into the neural network model. A model with a backdoor respond in the same way as the original model on normal input, but on a specific input, the responses are controlled by the backdoor. The backdoors can be triggered only when an input image contains a specific pattern and, from the model, it is difficult to find the pattern or even to detect whether such a backdoor exists. Most of these attacks occur during the generation or transmission of the models.
4.	Model Stealing:
In a model or training data extraction attack, an attacker analyzes the input, output, and other external information of a system to speculate on the parameters or training data of the model. Similar to the concept of Software-as-a-Service (SaaS) proposed by cloud service providers, AI-as-a-Service (AIaaS) is proposed by AI service providers to provide the services of model training, inference, etc. These services are open, and users can use open APIs to perform image and voice recognition. In this type of attack, an attacker invoked AIaaS APIs multiple times to steal AI models. These attacks create two problems. The first is the theft of intellectual property. Sample collection and model training require a lot of resources, so trained models are important intellectual property. The second problem is the black-box evasion attack mentioned earlier. Attackers can craft adversarial examples using extracted models.

Defense Against AI Security Attacks
Defense technologies for evasion attack: 
1. Network distillation: These technologies work by concatenating multiple DNNs in the model training stage, so that the classification result generated by one DNN is used for training the next DNN. Researchers found that the transfer of knowledge can reduce the sensitivity of an AI model to small perturbations and improve the robustness of the model. 
2. Adversarial training: This technology works by generating adversarial examples using known attack methods in the model training stage, adding the generated adversarial examples to the training set, and performing retraining one or multiple times to generate a new model resistant to attack perturbations. This technology enhances not only the robustness but also the accuracy and standardization of the new model, since the combination of various adversarial examples increases the training set data. 
3. Adversarial example detection: This technology identifies adversarial examples by adding an external detection model or a detection component of the original model in the model inference stage. Before an input sample arrives at the original model, the detection model determines whether the sample is adversarial. Alternatively, the detection model may extract related information at each layer of the original model to perform detection based on the extracted information. Detection models may identify adversarial examples based on different criteria. For example, the deterministic differences between input samples and normal data can be used as a criterion; and the distribution characteristics of adversarial examples and history of input samples can be used as the basis for identifying adversarial examples. 
4. Input reconstruction: This technology works by deforming input samples in the model inference stage to defend against evasion attacks. The deformed input does not affect the normal classification function of models. Input reconstruction can be implemented by adding noise, de-noising, or using an automatic encoder (autoencoder) to change an input sample. 
5. DNN verification: Similar to software verification analysis technologies, the DNN verification technology uses a solver to verify attributes of a DNN model. For example, a solver can verify that no adversarial examples exist within a specific perturbation range. However, DNN verification is an NP-complete problem, and the verification efficiency of the solver is low. The efficiency of DNN verification can be improved by tradeoffs and optimizations such as prioritizing model nodes in verification, sharing verification information, and performing region-based verification.

These defense technologies apply only to specific scenarios and cannot completely defend against all adversarial examples. In addition to these technologies, model robustness enhancement can be performed to improve the resistance to input perturbations while keeping model functions to defend against evasion attacks. Multiple defense technologies can be combined in parallel or serially to defend against evasion attacks. 

Defense technologies for poisoning attack: 
1. Training data filtering: This technology focuses on the control of training data sets and implements detection and purification to prevent poisoning attacks from affecting models. This method can be implemented by identifying possible poisoned data points based on label characteristics and filtering those points out during retraining or comparing models to minimize sample data exploitable in poisoning attacks and filtering that data out. 
2. Regression analysis: Based on statistical methods, this technology detects noise and abnormal values in data sets. This method can be implemented in multiple ways. For example, different loss functions can be defined for a model to check abnormal values, or the distribution characteristics of data can be used. 
3. Ensemble analysis: This technology emphasizes the use of multiple sub-models to improve an ML system’s ability to defend against poisoning attacks. When the system comprises multiple independent models that use different training data sets, the probability of the system being affected by poisoning attacks is reduced. An ML system’s overall ability to defend against poisoning attacks can be further enhanced by controlling the collection of training data, filtering data, and periodically retraining and updating models. 

Defense technologies for backdoor attack: 
1. Input pre-processing: This technology aims to filter out inputs that can trigger backdoors to minimize the risk of triggering backdoors and changing model inference results. 
2. Model pruning: This technology prunes off neurons of the original model while keeping normal functions to reduce the possibility that backdoor neurons work. Neurons constituting a backdoor can be removed using fine grained pruning to prevent backdoor attacks. 

Defense technologies for model stealing: 
1. Private Aggregation of Teacher Ensembles (PATE): This technology works by segmenting training data into multiple sets in the model training stage, each for training an independent DNN model. The independent DNN models are then used to jointly train a student model by voting. This technology ensures that the inference of the student model does not reveal the information of a particular training data set, thus ensuring the privacy of the training data. 
2. Differentially private protection: This technology adds noise to data or models by means of differential privacy in the model training stage. 
3. Model watermarking: This technology embeds special recognition neurons into the original model in the model training stage. These neurons enable a special input sample to check whether some other model was obtained by stealing the original model.

As described above, adversarial ML exists extensively. Evasion attacks, poisoning attacks, and all kinds of methods that take advantage of vulnerabilities and backdoors are not only accurate, but also have strong transferability, leading to high risks of misjudgment by AI models. Thus, in addition to defense against known attacks, the security of an AI model itself must be enhanced to avoid the damage caused by other potential attacks.

Model detectability: Like traditional program analysis in software engineering, AI models could also be checked with some adversarial detection technologies such as black-box and white-box testing methods to guarantee some degree of security. However, existing test tools are generally based on open data sets having limited samples and cannot cover many cases in realistic deployments. Moreover, adversarial training technologies would cause high performance overhead due to re-training. Therefore, when AI models are being deployed in any system, a large number of security tests need to be performed on DNN models. For instance, a pre-processing unit could be used to filter out malicious samples prior to feeding into the training model, or a post-processing unit could be added to check the integrity of the model output to further reduce false positives. With these methodologies, we could possibly enhance the robustness of AI systems before deployment. 

Model verifiability: DNN models work surprisingly better than traditional ML techniques (for example, providing higher classification rates and lower false positive rates). Thus, DNN models are widely used in image and voice recognition applications. However, caution needs to be taken when applying AI models in security and safety sensitive applications such as autonomous driving and medical auto-diagnosis. Certified verification of DNN models can ensure security to some extent. Model verification generally requires restricting the mapping between the input space and output space to determine whether the output is within a certain range. However, since statistical optimization-based learning and verification methods typically cannot traverse all data distributions, corner cases like adversarial samples would still exist. In this situation, it is relatively difficult to implement specific protection measures in actual deployments. Principled defense can only be resolved when the fundamental working principle of DNN models is fully understood. 

Model explainability: At present, most AI models are considered to be complicated black-box systems whose decision-making process, justification logic, and inference basis are hard to fully interpret. In some applications, such as playing chess and machine translation, we need to understand why machines make this or that decision to ensure better interaction between humans and machines. Nevertheless, the inexplainability of AI systems does not bring big problems in these applications. A translation machine can continue to be a complete complex black-box system as long as it provides good translation results, even though it does not explain why it translates one word into another. However, in some use cases, inexplainability tends to bring legal or business risks. For example, in insurance and loan analysis systems, if an AI system cannot provide the basis for its analysis results, it may be criticized as being discriminative; in healthcare systems, to accurately perform further processing based on AI analysis results, the basis of AI inference needs to be known. For instance, we hope that an AI system can analyze whether a patient has cancer, and the AI system needs to show how it draws the conclusion. Moreover, it is impossible to effectively design a secure model when its working principle is unknown. Enhancing the explainability of AI systems helps in analyzing their logic vulnerabilities or blind spots of data, thereby improving the security of the AI systems.

Model explainability can also be implemented in three phases: 
1. "Explainable data" before modeling: Because models are trained using data, efforts to explain the behavior of a model can start by analyzing the data used to train the model. If a few representative characteristics can be found from the training data, required characteristics can be selected to build the model during training. With these meaningful characteristics, the input and output of the model can be explained. 
2. Building an "explainable model": One method is to supplement the AI structure by combining it with traditional ML. This combination can balance the effectiveness of the learning result and the explainability of the learning model and provide a framework for explainable learning. A common important theoretical foundation of traditional ML methods is statistics. This approach has been widely used and provides sound explainability in many computer fields, such as natural language processing, voice recognition, image recognition, information retrieval, and biological information recognition. 
3. Explainability analysis of established models: This approach seeks to analyze the dependencies between the input, output, and intermediate information of AI models to analyze and verify the models’ logic. In the literature, there are both general model analysis methods applicable to multiple models, such as Local Interpretable Model-Agnostic Explanations (LIME), and specific model analysis methods that can analyze the construction of a specific model in depth. 

When an AI system is explainable, we can effectively verify and check it. By analyzing the logical relationship between modules of the AI system and input data, for example, we can confirm that the customer reimbursement capability is irrelevant to the gender and race of the customer. The explainability of an AI system ensures clearer logical relationships between input data and intermediate data; this is the other advantage of AI system explainability. We can identify illegitimate or attack data, or even fix or delete adversarial examples based on the self-consistency of data to improve model robustness. 

The EU General Data Protection Regulation (GDPR) mandates that AI system decisions must not be based on user racial or ethnic origins, political positions, religious beliefs, etc. Explainable AI systems can ensure that the analysis results they produce comply with the GDPR requirement, preventing users from becoming victims of "algorithm discrimination." In most AI systems, prejudice does not often lie in the algorithm itself, but in the data provided to machines. If the inputs contain biased data — for example, when a company’s HR department applies a subtle prejudice against female job seekers — the number of cases in which female job seekers are rejected will increase in the model, resulting in a gender imbalance. Even when gender is not an important characteristic of model training data, the data can amplify human prejudice in the AI model analysis conclusion. Governments usually need to verify the security, reliability, and explainability of AI-enabled systems. Only an explainable, verifiable, robust AI system can build confidence and trust in the public.

When developing AI systems, we must pay close attention to their potential security risks; strengthen prevention mechanisms and constraint conditions; minimize risks; and ensure AI’s secure, reliable, and controllable development. When applying AI models, we must analyze and determine the risks in using AI models based on the characteristics and architecture of specific services, and design a robust AI security architecture and deployment solution using security mechanisms involving isolation, detection, failsafe, and redundancy.

In autonomous driving, if an AI system incorrectly decides on critical operations such as braking, turning, and acceleration, the system may seriously endanger human life or property. Therefore, we must guarantee the security of AI systems for critical operations. Various security tests are surely important, but simulation cannot guarantee that AI systems will not go wrong in real scenarios. In many applications, it may be difficult to find an AI system that can give 100 percent correct answers every time. This underlying uncertainty makes the security design of the system architecture more important. The system must enable fallback to manual operation or other secure status when unable to make a deterministic decision. For example, if an AI-assisted medical system cannot provide a definite answer about the required medicine and dosage or detects possible attacks, it is better for the system to reply "Consult the doctor" than to provide an inaccurate prediction that may endanger patient health. For safety’s sake, correct use of the following security mechanisms based on business requirements are essential to ensure AI business security:
1. Isolation: To ensure stable operation, an AI system analyzes and identifies the optimal solution and sends it to the control system for verification and implementation. Generally, the security architecture must isolate functional modules and setup access control mechanisms between modules. The isolation of AI models can reduce the attack surface for AI inference, while the isolation of the integrated decision module can reduce attacks on the decision module. The output of AI inference can be imported into the integrated decision module as an auxiliary decision-making suggestion, and only authorized suggestions can enter the decision module.
2. Detection: Adopting continuous monitoring and with an attack-detection model in the main system architecture, it is possible to comprehensively analyze the network security status and estimate the current risk level. When the risk is high, the integrated decision system can reject the suggestion coming from the automatic system and hand over control to a person to ensure security under attacks. 
3. Failsafe: When a system needs to conduct critical operations such as AI-assisted autonomous driving or medical surgery, a multi-level security architecture is required to ensure the entire system security. The certainty of the inference results provided by the AI system must be analyzed. When the certainty of the result is lower than a certain threshold, the system falls back to conventional rule-based technologies or manual processing. 
4. Redundancy: Many business decisions and data are associated with each other. A feasible method to ensure the security of AI models is to analyze whether the association has been ruined. A multi-model architecture can be set up for critical applications, so that a mistake in one model does not keep the system from reaching a valid decision. In addition, the multi-model architecture can largely reduce the possibility of the system being fully compromised by a single attack, thereby improving the robustness of the entire system.
