Category: Data Infrastructure, Governance, Security
Topic: Data Mesh

The Data Mesh is one of three important emerging data architectures; the other two are Data Fabric and Data Lakehouse. Organisations need to clearly understand what each of them is, why they are important and how to implement them at scale, in a hybrid landscape. Modern Data Strategy will look at the organisation’s data sources, data consumption, analytics goals to determine the optimal architecture to maximize value of data. Data Mesh is a new way of creating and sharing ‘Data-as-a-Product’ by decentralizing data ownership and accountability. However, extensive research, thinking and decision making are required to define proper data domain strategy as a pre-requisite to Data Mesh implementation. Next critical business task is to define trustworthy and accurate data products that can be easily discovered and shared across organization
Data Mesh is ideally suited for organizations dealing with: 
1.	Constant change in the topology of their data landscape 
2.	Proliferation of data sources and consumers 
3.	Diversity in data transformation and processing needs 
4.	The need to respond to data-related change quickly If you have ongoing change and complexity in your data landscape, along with a proliferation of sources and consumers, and you are dissatisfied with the data and AI investment expended versus the results achieved, instituting a Data Mesh approach is seriously worth considering. 

Data Mesh is an architectural and organizational paradigm shift for how companies work with and share data internally within their organization and/ or externally with their business partners. A key component of this paradigm shift is treating data as a product—allowing vertical teams, “domains,” to build and share data products horizontally across your company. The benefits of this layered approach can be enormous; each vertical team builds relevant and valuable data products to be used and combined with other domains’ data products. It also allows decentralized creativity, flexibility, and utilization of data products while empowering centralized discovery and federated governance. 
Data Mesh can be thought of like an “app store,” except the apps are not apps at all, they are data products. And for data products to be “installed” and “run,” you need discovery, storage, compute, data definitions and documentation and, of course, governance and security. As an example, from the context of Healthcare and Life Sciences industry, “app store” can be a collection of data topics such as patient demographics, claims, clinical and real-world evidence data that help rapidly uncover patient/population level health insights, enable better patient/member experience, accelerate drug development, and more. e. Using our app store analogy again, you can build any app you want, but in order to monetize it, you must follow guidelines, publish to standards, and meet security controls before anyone can find or buy it.
The key components of Data Mesh are: 
1.	Domain-Centric Ownership 
The vertical teams, or the domains, that own the creation of the data products. They act as product managers and data engineers, owning their own data product roadmaps, pipelines, and data transformations, as well as documentation. 

For organisations that have or are in the process of data consolidation on the data lake, there would be an existing data lake or data platform team that is responsible for ingestion of the data to the data lake. The Data Platform Team is often a separate team in larger organizations that is responsible for the underlying data platform and governance. This team also assists in developing standards and templates to ensure that the Domain Data Teams are following the best practices and are productive on the data platform. The Domain Data Team is the team that uses a self-serve platform to build and consume data products.

2.	Self-Service Data Platform 
A distributed but interconnected set of compute, storage, tools and capabilities that avoids silos and enables distributed domain teams to build and exchange data products through ingestion, transformation, and provisioning of data. 
3.	Federated Governance and Security
Horizontal interoperability standards and policies, horizontal data governance policies, and vertical domain-specific governance policies. This is how the company can ensure data remains secure while still providing data product teams the freedom and power of decentralization. 
4.	Data-as-a-Product 
Data as a Product must be easily discoverable, subscribable, and understandable through documentation. Data as product supports wider thinking about the F.A.I.R qualities: Findable, Accessible, Interoperable and Reusable. A platform allows domain teams to operate independently and easily share data products with each other.

The first component of Data Mesh is an organizational change; the other three components also involve some organization change (people, process aspects) on top of the technology changes. Data Mesh architecture requires a strong change management process for an effective roll out.


STEPS TO CREATE A DATA MESH ARCHITECTURE
1.	Self-Service Data Platform 
Define Data Platform where data domain owners can access the data in a secure manner. Depending on where the organization is in the maturity of the implementation of their data strategy, different options are available to facilitate access to the data aligned to the data domain. If the organization has majority of the data consolidated on the data lake in different data zones, cloud data warehouse platform such as Snowflake can be used as the self service data platform. Alternatively, if the organization has adopted delta lake as the storage file format, data domains can access the domain data by querying the delta lake files from the raw zone or enriched or published zone. Lakehouse Federation capabilities enable organizations to create a highly scalable and performant data mesh architecture with unified governance. Alternatively, tools such as Prophecy can be used as the Self-Serve Platform that retrieves the data from the data lakehouse. If the organization is in the early stage of consolidating their data on the data lake or have data sources corresponding to a data domain that do not yet have the data on the data lake, then data virtualization tools can be leveraged to access the data directly from the transactional system or the system of record. Tools such as Databricks Lakehouse Federation in Unity Catalog allow customers to discover, query, and govern data across all of their data platforms from within Databricks without moving or copying the data first to the data lakehouse. While transactional system performance can be a concern when querying data for analytics in a data mesh architecture, careful design including caching, query optimization, and the use of replicated data storage and scalable data processing technologies can help mitigate these concerns and ensure that both operational and analytical needs are met efficiently. Tools such as StarBurst provide a platform with a rich ecosystem of connectors to connect to various data systems without needing the data to be consolidated on the data lake and also include data modelling capabilities to model the data corresponding to the data domain.

Companies adopting a Data Mesh architecture must have an analytics engine capable of federating across these different data sources. Starburst is the analytics engine for the Data Mesh architecture, providing a single point of access to distributed data and empowering self-service analytics for each of the business domains. Starburst is built on open-source Trino, a distributed engine that can execute SQL queries against data stored in a range of databases and file systems. With Starburst and Trino, teams can lower the total cost of their infrastructure and analytics investments, prevent vendor lock-in, and use the existing tools that work for their business so that they can concentrate on enabling faster time-to-insights. Trino’s open technology means that integration with other open technologies such as data catalogs and data discovery tools is simpler and reduces the total cost of ownership of the self-service data platform.

2.	Federated Governance and Security
Now that your data product teams have the autonomy of a self-service data platform, you must add access control and security. Complexity arises here because many data access control policies are not domain-specific and need to be enforced globally and consistently across domains, regardless of the data product. But this must be done in a way that also allows the data product teams to layer on additional domain specific controls. The following are key components of federated governance and security: 1. Ability to delegate and assign policy ownership to users globally or scoped more precisely to specific domain owners and their data products. 2. Ability to create a common taxonomy for how to describe and represent data through table and column tags and automatically tag said data. 3. Ability to author policies specific to your domain of control (global or domain-specific) without creating policy conflicts or disrupting existing policy. 4. Ability to detect and examine query activity against your data product(s).

Distributed access control is central to the adoption of Data Mesh. Data Mesh distributes data ownership to domain owners, so in order to facilitate data access for Data Mesh, access controls must also be distributed. This is accomplished by enabling domains to be autonomous with respect to access controls. Autonomy can be achieved through the use of approaches such as Policy as Code and attribute-based access control (ABAC), where the policies are associated with the Data Product. Traditional access control solutions are much less suitable, as they are often static, role-based models that lead to data policy duplication, data copies, maintenance complexity, and eventual evolution toward centralisation. Migrating to a distributed access control model allows you to empower data owners to take control of their Data Products, while ensuring data is accessed only by the right people, for the right reasons, at the right time.

How do organisations ensure they are ready for distributed access control in the Data Mesh architecture? Step one is to separate policy from platform. Much like the separation of compute from storage has allowed Data Mesh architectures to grow, separating data policy from platform enables the flexibility and scalability that Data Mesh requires. This approach allows you to associate policies based on data domains and Data Products that scale. As more domains within an organisation adopt Data Mesh, the policy landscape becomes increasingly complex with regulations, data use agreements, and data sharing requirements. Policies should therefore be created with the data product, not with the platform. Step two is ensuring that organisations with a dynamic access control solution can avoid that policy management from growing out of control. As more Data Products are created, policy implementation must be flexible to ensure that hardcoded values, like roles or custom SQL logic, do not introduce unnecessary maintenance overhead. Modern data access platforms provide scalable, flexible, and easy-to-use ways to manage data access and governance. Finally, data use and data sharing agreements are now becoming the standard in Data Mesh architectures. Every Data Mesh architecture will need the ability to assign data use agreements to Data Products. This ensures that Data Products are used appropriately under Zero Trust or on an as-needed basis, and allows organisations to future-proof their architecture from current and forthcoming requirements that must have purpose-based or use agreements in place.

Tools such as Immuta provides data teams one universal platform to control access to analytical data sets in the cloud. Immuta can automate access to data by discovering, protecting, and monitoring data.

3.	Data As A Product 
The final critical component of Data Mesh is data as a product. It is important to understand what a data product is. Different types of data products can exist in a Data Mesh including: 
a.	Physical data products. Physical data products are persisted datasets that have been produced, stored, and published in a data marketplace to make them available for consumption.
b.	Virtual data products. Virtual data products are virtual views that integrate data from one or more underlying data sources (including ready-made physical data products) on-demand, on a timer driven basis or on a continuous basis. They can also be materialised for performance. Again, they can be published in a data marketplace for people to discover, query and use.
c.	Stored queries. Stored queries are typically SQL queries that can be published as services and invoked on demand e.g., via a REST or GraphQL API. When executed, the stored query will then produce a data product and serve it. Stored queries can integrate data from one or more data sources including other data products.
d.	Analytical Products. BI reports, dashboards, predictive machine learning models, prescriptive machine learning models (e.g., recommendation engines), chat bots, automation bots are examples of analytical data products.

Once data product is created, it is possible for data domain teams to consume data products in another pipeline and integrate this data with other data domains to produce a new data product. This new data product can itself then be added to the Data Mesh. In that sense Data Mesh is a kind of bootstrapping capability where more and more data products are incrementally built over time and made available for others to consume and use. Therefore, as more data products become available, the time taken to deliver insights should get shorter because each new project doesn’t have to start from scratch to create data.

The data should be easy to discover, self-describing and accessible for use. Just like an app store, data products must be easily discoverable, subscribable, and understandable and provide a data portal user interface or a data sharing platform or marketplace to support data as a product. Key components of this data portal are that: 
A.	Data products are searchable by tag, name, or documentation content 
B.	Data product owners can fully document the data products to include documenting individual columns 
C.	Data products can be hidden from consumers based on policy 
D.	Consumers that don’t meet policy to gain access to a data product can be allowed access through manual just-in-time approvals (if prescribed in the policy).
Databricks Delta tables can also used as a data product that can be published and consumed, using multi-model access. The combination of Unity Catalog, Delta Sharing and Databricks Marketplace helps address the requirement of implementing a Data Mesh. Tools such as Prophecy provide a Self-Serve Platform built on top of the data lakehouse to build, publish and find data products. Data as a Product can also be published to a marketplace for data monetization. Data can be made discoverable to publishing the data in a data catalog.
