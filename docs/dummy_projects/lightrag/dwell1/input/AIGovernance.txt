Category: AI & ML foundations
Topic: AI governance

What is AI governance?
AI is increasingly being used as an automated decision-making system for businesses. Some of these are straightforward predictions that do not require human oversight. Others are critical decisions that are made with little human oversight. In these situations, the risk and impact of an inaccurate prediction is high. As organisations calibrate the extent of human involvement in AI, it is also crucial to define what AI governance means for them.
AI governance is a framework and process for organisations to ensure that their AI systems work as intended, in accordance to customer expectations, organisational goals and societal laws and norms. When integrated with other parts of the organisation, decision tradeoffs can be made in view of overall compliance and risk management perspectives.

A traditional data governance program oversees a range of activities, including data security, reference and master data management, data quality, data architecture, and metadata management. Now with growing adoption of data science, machine learning, and AI, there are new components that should also sit under the data governance umbrella. These are namely machine learning model management and Responsible AI governance,
What is need of AI Governance?
Just as the use of data is governed by a data governance program, the development and use of machine learning models in production requires clear, unambiguous policies, roles, standards, and metrics. A robust machine learning model management program would aim to answer questions such as: 
1.	Who is responsible for the performance and maintenance of production machine learning models? 
2.	How are machine learning models updated and/or refreshed to account for model drift (deterioration in the model’s performance)? 
3.	What performance metrics are measured when developing and selecting models, and what level of performance is acceptable to the business? 
4.	How are models monitored over time to detect model deterioration or unexpected, anomalous data and predictions? 
5.	How are models audited, and are they explainable to those outside of the team developing them?
How does AI ethics or Responsible AI relate to AI governance?
AI governance and AI ethics are often discussed in tandem. AI ethics refers to a set of moral principles that help define the boundaries and responsibilities for action, especially in highly ambiguous situations. The use of AI will invariably bring about ethical dilemmas. From the boundaries of privacy, to the impact of social media algorithms on our mental and emotional wellbeing, ethical decisions need to be made by leaders, not machines. This entails a moral judgement on what is good, and what is harmful. For example: How much should we trade off profit and transparency? How do we define a fair decision making process? How far should we trust AI systems to make important decisions without humans-in-the-loop? What features (e.g. gender, age, income) are justifiable in the delivery of differential treatment or services? What data is being chosen to train models, and does this data have pre-existing bias in and of itself? What are the protected characteristics that should be omitted from the model training process (such as ethnicity, gender, age, religion, etc.)? How do we account for and mitigate model bias and unfairness against certain groups? How do we respect the data privacy of our customers, employees, users, and citizens? How long can we legitimately retain data beyond its original intended use? Are the means by which we collect and store data in line not only with regulatory standards, but with our own company’s standards?
AI governance can help leaders make the implications of these ethical decisions more transparent, by designing metrics to evaluate ethical trade-offs. For example, we can compare a model designed to have maximum effectiveness but no fairness constraints, versus another model that is designed to be fair, to see the relative drop in effectiveness.
How does MLOps relate to AI governance? 
MLOps (a compound of “machine learning” and “operations”) has proved a useful foundation for the implementation of sound AI governance. MLOps is a machine learning practice that draws from DevOps approaches to increase visibility, automation and availability in machine learning systems. MLOps arose out of the recognition that it is challenging to build a performant machine learning engine and subsequently maintain them. 
MLOps encourages: 
1.	Visibility of metrics. Enabling the ability to monitor and understand the way your systems are functioning at every level. 
2.	Version control. Ensuring you have the ability to collaborate, iterate and roll-back where necessary. 
3.	Automation. Automating various tasks of data scientists and engineers so that they can focus on the creative aspects

Five Keys to Defining a Successful AI Governance Strategy
1.	A Top-Down And Bottom-Up Strategy 
Every AI governance program needs executive sponsorship. Without strong support from leadership, it is unlikely a company will make the right changes to improve data security, quality, and management. At the same time, individual teams have to take collective responsibility for the data they manage and the analysis they produce. There needs to be a culture of continuous improvement and ownership of data issues. This bottom-up approach can only be achieved in tandem with top-down communications and recognition of teams that have made real improvements and can serve as an example to the rest of the organization.

2.	Balance Between Governance and Enablement
Governance shouldn’t be a blocker to innovation; rather, it should enable and support innovation. That means in many cases, teams need to make distinctions between proof-of-concepts, self-service data initiatives, and industrialized data products, as well as the governance needs surrounding each. Space needs to be given for exploration and experimentation, but teams also need to make a clear decision about when self-service projects or proof-of-concepts should have the funding, testing, and assurance to become an industrialized, operationalized solution.

3.	Quality at its Heart 
In many companies, data products produced by data science and business intelligence teams have not had the same commitment to quality as traditional software development (through movements such as extreme programming and software craftsmanship). In many ways, this arose because five to ten years ago, data science was still a relatively new discipline, and practitioners were mostly working in experimental environments, not pushing to production. So while data science used to be the wild west, today, its adoption and importance has grown so much that standards of quality applied to software development need to be reapplied. Not only does the quality of the data itself matter now more than ever, but also data products need to have the same high standards of quality — through code review, testing and continuous integration/continuous development (CI/CD) — that traditional software does if the insights are to be trusted and adopted by the business at scale.

4.	Model Management 
As machine learning and deep learning models become more widespread in the decisions made across industries, model management is becoming a key factor in any AI Governance strategy. This is especially true today as the economic climate shifts, causing massive changes in underlying data and models that degrade or drift more quickly. Continuous monitoring, model refreshes and testing are needed to ensure the performance of models meet the needs of the business. To this end, MLOps is an attempt to take the best of DevOps processes from software development and apply them to data science.

5.	Transparency and Responsible AI 
Even if, per the third component, data scientists write tidy code and adhere to high quality standards, they are still giving away a certain level of control to complex algorithms. In other words, it’s not just about quality of data or code, but making sure that models do what they’re intended to do. There is growing scrutiny on decisions made by machine learning models, and rightly so. Models are making decisions that impact many people’s lives every day, so understanding the implications of the decisions they make and making the models explainable is essential (both for the people impacted and the companies producing them). Open source toolkits such as Aequitas, developed by the University of Chicago, make it simpler for machine learning developers, analysts, and policymakers to understand the types of bias that machine learning models bring. Similarly, Dataiku can compute and display subpopulation analyses as a part of its end-to-end data science, machine learning, and AI platform offering, which help assess if models behave identically across subpopulations. Of course, in any case, it’s always up to the individual organization to establish what’s considered fair (or not) for its particular use case.
