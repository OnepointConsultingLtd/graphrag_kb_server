Category: Analyse
Topic: NLP


NLP is a branch of machine learning and AI which deals with human language, and more specifically with bridging the gap between human communication and computer understanding. Its practical applications span from topic extraction from documents, to sentiment analysis of clients putting reviews in social media, to getting insights about the needs and the struggles of people calling customer support services, or even going as far as building near human conversational agents to offload these call centers, for instance. NLP sounds like a very niche thing, but it’s actually incredibly prevalent. You’ve probably encountered a natural language processing system in your day to day life without realizing it. Some common subfields of NLP are: 
1.	Question answering (search engines) 
2.	Speech recognition (Siri, Alexa) 
3.	Machine translation - translating from one language to another (Google Translate) 
4.	Information extraction - pulling relevant details from unstructured and/or structured data (like important info from health records, relevant news that could impact a trade for a trading algorithm, etc.) 
5.	Sentiment analysis - detecting the attitude (positive, negative, neutral) of a piece of text (used by businesses on their social media comments or for customer service, etc.) 

HOW DOES NLP WORK?
1. PRE-PROCESSING DATA The data must be cleaned and annotated (labeled) so that it can be processed by an algorithm. It’s also worth noting that newer techniques can leverage non-labelled data in pre-training models which would then be trained or fine tuned on labeled data (read more about this in the next section). Cleaning usually involves deconstructing the data into words or chunks of words (tokenization), removing parts of speech without any inherent meaning (like stop words such as a, the, an), making the data more uniform (like changing all words to lowercase), and grouping words into predefined categories such as the names of persons (entity extraction). All of this can be done using the spaCy library in Python. Annotation boils down to examining surrounding words and using language rules or statistics to tag parts of speech (similar to how we would use context clues to guess the meaning of a word). 
2. VECTORIZATION 
After preprocessing, the text data is transformed into numerical data, since machine learning models can only handle numerical input. Traditionally, the two main vectorization techniques that have been used most widely are Count Vectorization and Term Frequency-Inverse Document Frequency (TF-IDF). Count Vectorization involves counting the number of appearances of each word in a document or document section (i.e distinct text such as an article, book, a paragraph, etc.). The TF-IDF approach takes the logarithmic function of the size of the set of documents, and in how many documents a word appears. This is then multiplied by the term frequency to get a score. If the TF-IDF score is high, it means that it is good at discriminating between documents. This can be very useful, unlike Count Vectorization which only counts how many times a word occurs. Finally, a third technique called word embedding has nowadays become the dominant approach to vectorization. Embedding is a type of word representation that allows words with similar meaning to have a similar representation by mapping them to vectors of real numbers. Unlike older methods, word embeddings are able to represent implicit relationships between words that are useful when training on data that can benefit from contextual information.
3.TESTING 
Once a baseline has been created (the “rough draft” NLP model), its prediction accuracy is tested using a test subset. The model is built using the training subset and then tested on the testing subset to see if the model is generalizable-- we don’t want a model that only gives accurate predictions for one specific dataset!
