{
  "doc-e7b041ee744814e5cb2aceeed6a1459f": {
    "content": "Category: AI & ML foundations\nTopic: AI governance\n\nWhat is AI governance?\nAI is increasingly being used as an automated decision-making system for businesses. Some of these are straightforward predictions that do not require human oversight. Others are critical decisions that are made with little human oversight. In these situations, the risk and impact of an inaccurate prediction is high. As organisations calibrate the extent of human involvement in AI, it is also crucial to define what AI governance means for them.\nAI governance is a framework and process for organisations to ensure that their AI systems work as intended, in accordance to customer expectations, organisational goals and societal laws and norms. When integrated with other parts of the organisation, decision tradeoffs can be made in view of overall compliance and risk management perspectives.\n\nA traditional data governance program oversees a range of activities, including data security, reference and master data management, data quality, data architecture, and metadata management. Now with growing adoption of data science, machine learning, and AI, there are new components that should also sit under the data governance umbrella. These are namely machine learning model management and Responsible AI governance,\nWhat is need of AI Governance?\nJust as the use of data is governed by a data governance program, the development and use of machine learning models in production requires clear, unambiguous policies, roles, standards, and metrics. A robust machine learning model management program would aim to answer questions such as: \n1.\tWho is responsible for the performance and maintenance of production machine learning models? \n2.\tHow are machine learning models updated and/or refreshed to account for model drift (deterioration in the model’s performance)? \n3.\tWhat performance metrics are measured when developing and selecting models, and what level of performance is acceptable to the business? \n4.\tHow are models monitored over time to detect model deterioration or unexpected, anomalous data and predictions? \n5.\tHow are models audited, and are they explainable to those outside of the team developing them?\nHow does AI ethics or Responsible AI relate to AI governance?\nAI governance and AI ethics are often discussed in tandem. AI ethics refers to a set of moral principles that help define the boundaries and responsibilities for action, especially in highly ambiguous situations. The use of AI will invariably bring about ethical dilemmas. From the boundaries of privacy, to the impact of social media algorithms on our mental and emotional wellbeing, ethical decisions need to be made by leaders, not machines. This entails a moral judgement on what is good, and what is harmful. For example: How much should we trade off profit and transparency? How do we define a fair decision making process? How far should we trust AI systems to make important decisions without humans-in-the-loop? What features (e.g. gender, age, income) are justifiable in the delivery of differential treatment or services? What data is being chosen to train models, and does this data have pre-existing bias in and of itself? What are the protected characteristics that should be omitted from the model training process (such as ethnicity, gender, age, religion, etc.)? How do we account for and mitigate model bias and unfairness against certain groups? How do we respect the data privacy of our customers, employees, users, and citizens? How long can we legitimately retain data beyond its original intended use? Are the means by which we collect and store data in line not only with regulatory standards, but with our own company’s standards?\nAI governance can help leaders make the implications of these ethical decisions more transparent, by designing metrics to evaluate ethical trade-offs. For example, we can compare a model designed to have maximum effectiveness but no fairness constraints, versus another model that is designed to be fair, to see the relative drop in effectiveness.\nHow does MLOps relate to AI governance? \nMLOps (a compound of “machine learning” and “operations”) has proved a useful foundation for the implementation of sound AI governance. MLOps is a machine learning practice that draws from DevOps approaches to increase visibility, automation and availability in machine learning systems. MLOps arose out of the recognition that it is challenging to build a performant machine learning engine and subsequently maintain them. \nMLOps encourages: \n1.\tVisibility of metrics. Enabling the ability to monitor and understand the way your systems are functioning at every level. \n2.\tVersion control. Ensuring you have the ability to collaborate, iterate and roll-back where necessary. \n3.\tAutomation. Automating various tasks of data scientists and engineers so that they can focus on the creative aspects\n\nFive Keys to Defining a Successful AI Governance Strategy\n1.\tA Top-Down And Bottom-Up Strategy \nEvery AI governance program needs executive sponsorship. Without strong support from leadership, it is unlikely a company will make the right changes to improve data security, quality, and management. At the same time, individual teams have to take collective responsibility for the data they manage and the analysis they produce. There needs to be a culture of continuous improvement and ownership of data issues. This bottom-up approach can only be achieved in tandem with top-down communications and recognition of teams that have made real improvements and can serve as an example to the rest of the organization.\n\n2.\tBalance Between Governance and Enablement\nGovernance shouldn’t be a blocker to innovation; rather, it should enable and support innovation. That means in many cases, teams need to make distinctions between proof-of-concepts, self-service data initiatives, and industrialized data products, as well as the governance needs surrounding each. Space needs to be given for exploration and experimentation, but teams also need to make a clear decision about when self-service projects or proof-of-concepts should have the funding, testing, and assurance to become an industrialized, operationalized solution.\n\n3.\tQuality at its Heart \nIn many companies, data products produced by data science and business intelligence teams have not had the same commitment to quality as traditional software development (through movements such as extreme programming and software craftsmanship). In many ways, this arose because five to ten years ago, data science was still a relatively new discipline, and practitioners were mostly working in experimental environments, not pushing to production. So while data science used to be the wild west, today, its adoption and importance has grown so much that standards of quality applied to software development need to be reapplied. Not only does the quality of the data itself matter now more than ever, but also data products need to have the same high standards of quality — through code review, testing and continuous integration/continuous development (CI/CD) — that traditional software does if the insights are to be trusted and adopted by the business at scale.\n\n4.\tModel Management \nAs machine learning and deep learning models become more widespread in the decisions made across industries, model management is becoming a key factor in any AI Governance strategy. This is especially true today as the economic climate shifts, causing massive changes in underlying data and models that degrade or drift more quickly. Continuous monitoring, model refreshes and testing are needed to ensure the performance of models meet the needs of the business. To this end, MLOps is an attempt to take the best of DevOps processes from software development and apply them to data science.\n\n5.\tTransparency and Responsible AI \nEven if, per the third component, data scientists write tidy code and adhere to high quality standards, they are still giving away a certain level of control to complex algorithms. In other words, it’s not just about quality of data or code, but making sure that models do what they’re intended to do. There is growing scrutiny on decisions made by machine learning models, and rightly so. Models are making decisions that impact many people’s lives every day, so understanding the implications of the decisions they make and making the models explainable is essential (both for the people impacted and the companies producing them). Open source toolkits such as Aequitas, developed by the University of Chicago, make it simpler for machine learning developers, analysts, and policymakers to understand the types of bias that machine learning models bring. Similarly, Dataiku can compute and display subpopulation analyses as a part of its end-to-end data science, machine learning, and AI platform offering, which help assess if models behave identically across subpopulations. Of course, in any case, it’s always up to the individual organization to establish what’s considered fair (or not) for its particular use case."
  },
  "doc-f00b18a9122e987fc337a4ff8d0b57c2": {
    "content": "Category: AI & ML foundations\nTopic: AI Security\n\nThe large-scale popularization and advancement of AI require strong security assurance. AI security model focuses on two types of AI attacks and defenses. First, affecting the correctness of AI decisions: An attacker can sabotage or control an AI system or intentionally change the input so that the system makes a decision desired by the attacker. Second, attackers may steal the confidential data used to train an AI system or extract the AI model. With these problems in mind, AI system security should be dealt with from three aspects: AI attack mitigation, AI model security, and AI architecture security. In addition, the transparency and explainability of AI are the foundation of security. An AI system that is not transparent or explainable cannot undertake critical tasks involving personal safety and public safety.\n\nIn order to tackle the new AI security challenges, three layers of defense for deploying AI systems are recommended: \n1. Attack mitigation: Design defense mechanisms for known attacks. \n2. Model security: Enhance model robustness by various mechanisms such as model verification.\n3. Architecture security: Build a secure architecture with multiple security mechanisms to ensure business security.\n\nAI has great potential to build a better, smarter world, but at the same time faces severe security risks. Due to the lack of security consideration at the early development of AI algorithms, attackers are able to manipulate the inference results in ways that lead to misjudgment. In critical domains such as healthcare, transportation, and surveillance, security risks can be devastating. Successful attacks on AI systems can result in property loss or endanger personal safety. AI security risks exist not only in theoretical analyses but also in AI deployments. For instance, attackers can craft files to bypass AI-based detection tools or add noise to smart home voice control command to invoke malicious applications. Attackers can also tamper with data returned by a terminal or deliberately engage in malicious dialogs with a chat robot to cause a prediction error in the backend AI system. It is even possible to apply small stickers on traffic signs or vehicles that cause false inferences by autonomous vehicles.\n\n\nTo mitigate these AI security risks, AI system design must overcome five security challenges: \n1.\tSoftware and hardware security: The code of applications, models, platforms, and chips may have vulnerabilities or backdoors that attackers can exploit. Further, attackers may implant backdoors in models to launch advanced attacks. Due to the inexplainability of AI models, the backdoors are difficult to discover. \n2.\tData integrity: Attackers can inject malicious data in the training stage to affect the inference capability of AI models or add a small perturbation to input samples in the inference stage to change the inference result. \n3.\tModel confidentiality: Service providers generally want to provide only query services without exposing the training models. However, an attacker may create a clone model through a number of queries. \n4.\tModel robustness: Training samples typically do not cover all possible corner cases, resulting in the insufficiency of robustness. Therefore the model may fail to provide correct inference on adversarial examples. \n5.\tData privacy: For scenarios in which users provide training data, attackers can repeatedly query a trained model to obtain users’ private information.\n\nTypical AI Security Attacks\n1.\tEvasion Attack:\nIn an evasion attack, an attacker modifies input data so that the AI model cannot correctly identify the input. \nAdversarial examples: Studies show that deep learning systems can be easily affected by well-crafted input samples, which are called adversarial examples. They are usually obtained by adding small perturbations into the original legitimate samples. These changes are not noticeable to human eyes but greatly affect the output of deep learning models.\nTransferability and black-box attacks: To generate adversarial examples, attackers need to obtain AI model parameters, but these parameters are difficult to obtain in some scenarios. An adversarial example generated against a model can also deceive another model as long as the training data of the two models are the same. Attackers can exploit this transferability to launch black-box attacks without knowing AI model parameters. To do that, an attacker queries a model multiple times, uses the query results to train a \"substitute model,\" and finally uses the substitute model to generate adversarial examples, which can be used to deceive the original model.\n2.\tPoisoning:\nAI systems are usually retrained using new data collected after deployment to adapt to changes in input distribution. For example, an Intrusion Detection System (IDS) continuously collects samples on a network and retrains models to detect new attacks. In a poisoning attack, the attacker may inject carefully crafted samples to contaminate the training data in a way that eventually impairs the normal functions of the AI system — for example, escaping AI security detection. Deep learning requires a large number of training samples, so it is difficult to guarantee the quality of samples. Mixing a small number of adversarial examples with training samples can significantly affect the accuracy of AI models. For example, by adding 8 percent of malicious training data, attackers can cause a 75-percent change of the dosages suggested by models for half of the patients. There are three kinds of poisoning attacks: optimal gradient attack, global optimum attack, and statistical optimization attack.\n3.\tBackdoor:\nLike traditional programs, AI models can be embedded with backdoors. Only the person who makes backdoors knows how to trigger them; other people do not know of their existence nor can trigger them. Unlike traditional programs, a neural network model only consists of a set of parameters, without source code. Therefore, backdoors in AI models are harder to detect than in traditional programs. These backdoors are typically implanted by adding some specific neuron into the neural network model. A model with a backdoor respond in the same way as the original model on normal input, but on a specific input, the responses are controlled by the backdoor. The backdoors can be triggered only when an input image contains a specific pattern and, from the model, it is difficult to find the pattern or even to detect whether such a backdoor exists. Most of these attacks occur during the generation or transmission of the models.\n4.\tModel Stealing:\nIn a model or training data extraction attack, an attacker analyzes the input, output, and other external information of a system to speculate on the parameters or training data of the model. Similar to the concept of Software-as-a-Service (SaaS) proposed by cloud service providers, AI-as-a-Service (AIaaS) is proposed by AI service providers to provide the services of model training, inference, etc. These services are open, and users can use open APIs to perform image and voice recognition. In this type of attack, an attacker invoked AIaaS APIs multiple times to steal AI models. These attacks create two problems. The first is the theft of intellectual property. Sample collection and model training require a lot of resources, so trained models are important intellectual property. The second problem is the black-box evasion attack mentioned earlier. Attackers can craft adversarial examples using extracted models.\n\nDefense Against AI Security Attacks\nDefense technologies for evasion attack: \n1. Network distillation: These technologies work by concatenating multiple DNNs in the model training stage, so that the classification result generated by one DNN is used for training the next DNN. Researchers found that the transfer of knowledge can reduce the sensitivity of an AI model to small perturbations and improve the robustness of the model. \n2. Adversarial training: This technology works by generating adversarial examples using known attack methods in the model training stage, adding the generated adversarial examples to the training set, and performing retraining one or multiple times to generate a new model resistant to attack perturbations. This technology enhances not only the robustness but also the accuracy and standardization of the new model, since the combination of various adversarial examples increases the training set data. \n3. Adversarial example detection: This technology identifies adversarial examples by adding an external detection model or a detection component of the original model in the model inference stage. Before an input sample arrives at the original model, the detection model determines whether the sample is adversarial. Alternatively, the detection model may extract related information at each layer of the original model to perform detection based on the extracted information. Detection models may identify adversarial examples based on different criteria. For example, the deterministic differences between input samples and normal data can be used as a criterion; and the distribution characteristics of adversarial examples and history of input samples can be used as the basis for identifying adversarial examples. \n4. Input reconstruction: This technology works by deforming input samples in the model inference stage to defend against evasion attacks. The deformed input does not affect the normal classification function of models. Input reconstruction can be implemented by adding noise, de-noising, or using an automatic encoder (autoencoder) to change an input sample. \n5. DNN verification: Similar to software verification analysis technologies, the DNN verification technology uses a solver to verify attributes of a DNN model. For example, a solver can verify that no adversarial examples exist within a specific perturbation range. However, DNN verification is an NP-complete problem, and the verification efficiency of the solver is low. The efficiency of DNN verification can be improved by tradeoffs and optimizations such as prioritizing model nodes in verification, sharing verification information, and performing region-based verification.\n\nThese defense technologies apply only to specific scenarios and cannot completely defend against all adversarial examples. In addition to these technologies, model robustness enhancement can be performed to improve the resistance to input perturbations while keeping model functions to defend against evasion attacks. Multiple defense technologies can be combined in parallel or serially to defend against evasion attacks. \n\nDefense technologies for poisoning attack: \n1. Training data filtering: This technology focuses on the control of training data sets and implements detection and purification to prevent poisoning attacks from affecting models. This method can be implemented by identifying possible poisoned data points based on label characteristics and filtering those points out during retraining or comparing models to minimize sample data exploitable in poisoning attacks and filtering that data out. \n2. Regression analysis: Based on statistical methods, this technology detects noise and abnormal values in data sets. This method can be implemented in multiple ways. For example, different loss functions can be defined for a model to check abnormal values, or the distribution characteristics of data can be used. \n3. Ensemble analysis: This technology emphasizes the use of multiple sub-models to improve an ML system’s ability to defend against poisoning attacks. When the system comprises multiple independent models that use different training data sets, the probability of the system being affected by poisoning attacks is reduced. An ML system’s overall ability to defend against poisoning attacks can be further enhanced by controlling the collection of training data, filtering data, and periodically retraining and updating models. \n\nDefense technologies for backdoor attack: \n1. Input pre-processing: This technology aims to filter out inputs that can trigger backdoors to minimize the risk of triggering backdoors and changing model inference results. \n2. Model pruning: This technology prunes off neurons of the original model while keeping normal functions to reduce the possibility that backdoor neurons work. Neurons constituting a backdoor can be removed using fine grained pruning to prevent backdoor attacks. \n\nDefense technologies for model stealing: \n1. Private Aggregation of Teacher Ensembles (PATE): This technology works by segmenting training data into multiple sets in the model training stage, each for training an independent DNN model. The independent DNN models are then used to jointly train a student model by voting. This technology ensures that the inference of the student model does not reveal the information of a particular training data set, thus ensuring the privacy of the training data. \n2. Differentially private protection: This technology adds noise to data or models by means of differential privacy in the model training stage. \n3. Model watermarking: This technology embeds special recognition neurons into the original model in the model training stage. These neurons enable a special input sample to check whether some other model was obtained by stealing the original model.\n\nAs described above, adversarial ML exists extensively. Evasion attacks, poisoning attacks, and all kinds of methods that take advantage of vulnerabilities and backdoors are not only accurate, but also have strong transferability, leading to high risks of misjudgment by AI models. Thus, in addition to defense against known attacks, the security of an AI model itself must be enhanced to avoid the damage caused by other potential attacks.\n\nModel detectability: Like traditional program analysis in software engineering, AI models could also be checked with some adversarial detection technologies such as black-box and white-box testing methods to guarantee some degree of security. However, existing test tools are generally based on open data sets having limited samples and cannot cover many cases in realistic deployments. Moreover, adversarial training technologies would cause high performance overhead due to re-training. Therefore, when AI models are being deployed in any system, a large number of security tests need to be performed on DNN models. For instance, a pre-processing unit could be used to filter out malicious samples prior to feeding into the training model, or a post-processing unit could be added to check the integrity of the model output to further reduce false positives. With these methodologies, we could possibly enhance the robustness of AI systems before deployment. \n\nModel verifiability: DNN models work surprisingly better than traditional ML techniques (for example, providing higher classification rates and lower false positive rates). Thus, DNN models are widely used in image and voice recognition applications. However, caution needs to be taken when applying AI models in security and safety sensitive applications such as autonomous driving and medical auto-diagnosis. Certified verification of DNN models can ensure security to some extent. Model verification generally requires restricting the mapping between the input space and output space to determine whether the output is within a certain range. However, since statistical optimization-based learning and verification methods typically cannot traverse all data distributions, corner cases like adversarial samples would still exist. In this situation, it is relatively difficult to implement specific protection measures in actual deployments. Principled defense can only be resolved when the fundamental working principle of DNN models is fully understood. \n\nModel explainability: At present, most AI models are considered to be complicated black-box systems whose decision-making process, justification logic, and inference basis are hard to fully interpret. In some applications, such as playing chess and machine translation, we need to understand why machines make this or that decision to ensure better interaction between humans and machines. Nevertheless, the inexplainability of AI systems does not bring big problems in these applications. A translation machine can continue to be a complete complex black-box system as long as it provides good translation results, even though it does not explain why it translates one word into another. However, in some use cases, inexplainability tends to bring legal or business risks. For example, in insurance and loan analysis systems, if an AI system cannot provide the basis for its analysis results, it may be criticized as being discriminative; in healthcare systems, to accurately perform further processing based on AI analysis results, the basis of AI inference needs to be known. For instance, we hope that an AI system can analyze whether a patient has cancer, and the AI system needs to show how it draws the conclusion. Moreover, it is impossible to effectively design a secure model when its working principle is unknown. Enhancing the explainability of AI systems helps in analyzing their logic vulnerabilities or blind spots of data, thereby improving the security of the AI systems.\n\nModel explainability can also be implemented in three phases: \n1. \"Explainable data\" before modeling: Because models are trained using data, efforts to explain the behavior of a model can start by analyzing the data used to train the model. If a few representative characteristics can be found from the training data, required characteristics can be selected to build the model during training. With these meaningful characteristics, the input and output of the model can be explained. \n2. Building an \"explainable model\": One method is to supplement the AI structure by combining it with traditional ML. This combination can balance the effectiveness of the learning result and the explainability of the learning model and provide a framework for explainable learning. A common important theoretical foundation of traditional ML methods is statistics. This approach has been widely used and provides sound explainability in many computer fields, such as natural language processing, voice recognition, image recognition, information retrieval, and biological information recognition. \n3. Explainability analysis of established models: This approach seeks to analyze the dependencies between the input, output, and intermediate information of AI models to analyze and verify the models’ logic. In the literature, there are both general model analysis methods applicable to multiple models, such as Local Interpretable Model-Agnostic Explanations (LIME), and specific model analysis methods that can analyze the construction of a specific model in depth. \n\nWhen an AI system is explainable, we can effectively verify and check it. By analyzing the logical relationship between modules of the AI system and input data, for example, we can confirm that the customer reimbursement capability is irrelevant to the gender and race of the customer. The explainability of an AI system ensures clearer logical relationships between input data and intermediate data; this is the other advantage of AI system explainability. We can identify illegitimate or attack data, or even fix or delete adversarial examples based on the self-consistency of data to improve model robustness. \n\nThe EU General Data Protection Regulation (GDPR) mandates that AI system decisions must not be based on user racial or ethnic origins, political positions, religious beliefs, etc. Explainable AI systems can ensure that the analysis results they produce comply with the GDPR requirement, preventing users from becoming victims of \"algorithm discrimination.\" In most AI systems, prejudice does not often lie in the algorithm itself, but in the data provided to machines. If the inputs contain biased data — for example, when a company’s HR department applies a subtle prejudice against female job seekers — the number of cases in which female job seekers are rejected will increase in the model, resulting in a gender imbalance. Even when gender is not an important characteristic of model training data, the data can amplify human prejudice in the AI model analysis conclusion. Governments usually need to verify the security, reliability, and explainability of AI-enabled systems. Only an explainable, verifiable, robust AI system can build confidence and trust in the public.\n\nWhen developing AI systems, we must pay close attention to their potential security risks; strengthen prevention mechanisms and constraint conditions; minimize risks; and ensure AI’s secure, reliable, and controllable development. When applying AI models, we must analyze and determine the risks in using AI models based on the characteristics and architecture of specific services, and design a robust AI security architecture and deployment solution using security mechanisms involving isolation, detection, failsafe, and redundancy.\n\nIn autonomous driving, if an AI system incorrectly decides on critical operations such as braking, turning, and acceleration, the system may seriously endanger human life or property. Therefore, we must guarantee the security of AI systems for critical operations. Various security tests are surely important, but simulation cannot guarantee that AI systems will not go wrong in real scenarios. In many applications, it may be difficult to find an AI system that can give 100 percent correct answers every time. This underlying uncertainty makes the security design of the system architecture more important. The system must enable fallback to manual operation or other secure status when unable to make a deterministic decision. For example, if an AI-assisted medical system cannot provide a definite answer about the required medicine and dosage or detects possible attacks, it is better for the system to reply \"Consult the doctor\" than to provide an inaccurate prediction that may endanger patient health. For safety’s sake, correct use of the following security mechanisms based on business requirements are essential to ensure AI business security:\n1. Isolation: To ensure stable operation, an AI system analyzes and identifies the optimal solution and sends it to the control system for verification and implementation. Generally, the security architecture must isolate functional modules and setup access control mechanisms between modules. The isolation of AI models can reduce the attack surface for AI inference, while the isolation of the integrated decision module can reduce attacks on the decision module. The output of AI inference can be imported into the integrated decision module as an auxiliary decision-making suggestion, and only authorized suggestions can enter the decision module.\n2. Detection: Adopting continuous monitoring and with an attack-detection model in the main system architecture, it is possible to comprehensively analyze the network security status and estimate the current risk level. When the risk is high, the integrated decision system can reject the suggestion coming from the automatic system and hand over control to a person to ensure security under attacks. \n3. Failsafe: When a system needs to conduct critical operations such as AI-assisted autonomous driving or medical surgery, a multi-level security architecture is required to ensure the entire system security. The certainty of the inference results provided by the AI system must be analyzed. When the certainty of the result is lower than a certain threshold, the system falls back to conventional rule-based technologies or manual processing. \n4. Redundancy: Many business decisions and data are associated with each other. A feasible method to ensure the security of AI models is to analyze whether the association has been ruined. A multi-model architecture can be set up for critical applications, so that a mistake in one model does not keep the system from reaching a valid decision. In addition, the multi-model architecture can largely reduce the possibility of the system being fully compromised by a single attack, thereby improving the robustness of the entire system."
  },
  "doc-516f77d958449cf06b10017bb25ef4bc": {
    "content": "Category: Consume\nTopic: consumption use cases\n\nConsumption use cases for published data in a data lake refer to the various ways organizations and users can access, analyze, and derive value from the data stored in the data lake. Common consumption use cases for published data in a data lake include descriptive analytics by building dashboards and scheduling reports using BI tools. Data analysts and scientists can explore and discover new patterns, trends, and anomalies in the data on the data lake. Exploratory data analysis often involves ad-hoc querying, data profiling, and data visualization. Data scientists and machine learning engineers can access raw, enriched and published data in the data lake to build and train machine learning models, perform data exploration, and develop predictive analytics solutions. Popular tools like Jupyter notebooks, AI/ML frameworks such as TensorFlow, and PyTorch are commonly used. \n\nEmerging use cases include building Customer 360, recommendation engines, digital twins and data monetization. Merging or joining data sets on data lakes for building a 360-degree view of customers, often referred to as \"Customer 360,\" can provide organizations with valuable insights into their customers' behavior, preferences, and needs. Using a data lake for digital twins involves leveraging the data storage and processing capabilities of the data lake to create and manage digital representations of physical objects or systems. Digital twins are virtual replicas of real-world entities, providing a powerful framework for simulation, analysis, and monitoring. Digital twins powered by data lakes are particularly valuable in industries such as manufacturing, healthcare, energy, transportation, and construction. They enable organizations to gain a deeper understanding of their physical assets, improve operational efficiency, reduce downtime, and enhance decision-making by leveraging real-time data and simulations. \n\nData monetization refers to the process of generating revenue or deriving economic value from the data assets an organization possesses. In today's data-driven world, organizations accumulate vast amounts of data on the data lake, and data monetization strategies allow them to leverage this data or information or insights for financial gain or strategic advantages. \nData monetization offers substantial benefits, but it also presents several challenges that organizations must address such as compliance with data privacy regulations, such as GDPR and CCPA and determining the right monetization strategy and pricing model for data. Data metering, which involves measuring and tracking data usage, can help overcome some of these challenges. By providing visibility into data usage, access patterns, and data quality, organizations can make informed decisions, enforce governance policies, and ensure compliance. Additionally, data metering enables organizations to better understand customer behavior and optimize their monetization pricing strategies, leading to more successful and profitable data monetization initiatives. \n\nData mesh can play a significant role in driving data monetization by decentralizing ownership, emphasizing data productization, providing robust metadata and discovery capabilities, and promoting self-service access while maintaining governance and control. This approach is designed to make data more accessible, manageable, and valuable for organizations with complex data ecosystems. \n\nTransforming a data lake into a real-time data hub involves implementing low latency event drive data pipelines for ingestion and exposing curated published datasets as a service. Securely exposing real-time data using API gateway enables users and applications to access and consume data in real time.\n\nBuilding conversational AI applications using a large language model (LLM) can be enhanced by integrating data from a data lake. Data lakes can serve as a valuable source of information for training and enhancing the capabilities of conversational AI models."
  },
  "doc-790e2564c40a86b105e00615f7628565": {
    "content": "Category: Produce & Aquire\nTopic: Data Sources Trends\n\nData can be classified according to various attributes of data such as structured/semi-structured/un-\nstructured data based on how the data is structured or how the data entities are related or structured\nrelative to each other such as relational data. Based on the how fast the data is changing, data can be\nclassified as static data such as reference data, slowly changing data such as Master Data and slow\nchanging dimensional data, or volatile data such as time series data and transactional data from Point of\nSale (POS) systems. Similarly based on the velocity or speed of movement of data or how fast volatile\ndata is moved from the source to the destination, data can be classified as fast data that includes\nstreaming data. Streaming data could include social media data or data from IoT sensor data. Big Data\nrefers to not only large volumes of data but also includes fast data that becomes large volume when\naccumulated over period of data. Big data includes variety of data including structure, semi-structured\nand unstructured data. Based on how the data is ageing from the time when the data was generated,\ndata can be classified as operational data or historical data. Based on how frequently the data is\nconsumed, data can also be classified as hot, warm, and cold. Often, data within a system in an\norganization is not exploited for analytics and such data is referred as dark data.\nModern Data Strategy is no longer limited to Big Data but is agnostic of the type of data and includes\nANY data whether its fast data or slow data, small data, or big data, transactional, operational, or\nhistorical data. There is more and more emphasis on including dark data in scope and bringing it to the\ndata lake. Adoption of LLM for enterprises could hugely benefit from modern data strategy since it\nincludes consolidation of unstructured data on the data lake.\n\nWe live in a data-saturated world. Billions of interconnected devices communicate with countless cloud services. We accumulate data from server log files, GPS networks, security tools, call records, web traffic and more. Every digital transaction, from backend handoffs to the customer’s fingertips, is catalogued. Everything from the contents of a warehouse’s shelves to the temperature of our server rooms to the time and location of every login to our secure networks is recorded and stored somewhere. Most of it, today, is unstructured, untagged, untapped. In a word, useless.\n\nDark data represents data not collected and therefore not used. Considering that at least 80%\nof data is dark,it represents the great hidden resource that flows untapped through major\norganizations. Leveraging dark data in the data integration processes is a challenge as the\nmost sophisticated data sources—such as network transactions, IoT, mobility, Wi-Fi, or\nindustrial networks—require an advanced engine specially built for the purpose.\nDark data may be the biggest untapped resource in business today. Dark data, which includes the “data exhaust” generated as a byproduct of our online lives, is all the unknown and untapped data across your organization, generated by systems, devices and interactions. Maybe it’s siloed off somewhere; maybe the format or metadata is inconsistent. Maybe no one’s figured out what to do with it. Maybe you literally don’t know it exists. Most organizations struggle to capitalize on the full potential of their data. A full third of respondents report that more than 75 percent of their organization’s data is dark. Just 11 percent — one in nine — report that less than a quarter of their organization’s data is dark. Inadequate processes, resources and technology hamper the intelligent use of dark data. Neglected by business and IT managers, dark data is an underused asset that demands a more sophisticated approach to how organizations collect, manage and analyze information. Yet respondents also voiced hesitance about diving in. It’s a challenge, and an opportunity, of surprising scale. Machine data, a major source of dark data, is growing much faster than traditional organizational data, with an accelerating importance to decision making and organizational success. And because dark data can be a powerful fuel for artificial intelligence, organizations that fail to tap its power will also fail to keep up with, much less surpass, their competitors. And competition is global. Organizations must grasp the opportunities and confront the challenges of dark data — through greater strategic thinking, targeted technology investment, and more energetic and comprehensive skills training — to take full advantage of the next data revolution. Organizations need to think now about how to bring dark data into the light. All the unknown and untapped data across your company, generated by systems, devices and interactions. \n\nWhy Data Stays Dark? \nDark data contains records of all the activity and behavior of customers, users, transactions, applications, servers, networks and mobile devices. It includes configurations, message queues, the output of diagnostic commands, call detail records, sensor data from industrial systems, and more. Dark data, in short, is any data that isn’t being used. That includes the many types of data generated by an organization’s systems and applications, from machine data to server log files to customer and user data to sentiment analysis derived from social media. It’s the byproduct of day-to-day business activity, both within an organization and across the ecosystem of customers, partners and suppliers. It can be data that is considered too old or outdated to provide value, data in a format that can’t be accessed with the tools available to the organization, incomplete data or duplicate data — any data that needs to be “cleaned” before it can be used. Often, organizations ignore potentially valuable data because they don’t have the time or resources to prepare it for use. Or they may not understand its full potential. Or they may be bogged down in the status quo, meeting day-to-day requirements rather than looking ahead for opportunities. By definition, an organization’s dark data is the most difficult to access. How do organizations improve their access to dark data and the insights it holds? Traditionally, the quick and easy (if expensive) solution was to hire a consultancy and have them explore dark data in all the data sources by doing a data scan. Today’s dark data could one day be an accelerant for even greater AI performance. Thus, the advent of AI and the value of dark data go hand-in-hand. Dark data provides an enormous, untapped resource of information that AI can analyze. And AI-powered analytics tools can help make dark data ready for analysis on a scale that would be impossible with current methods. On the one hand, dark data and artificial intelligence hold almost unlimited potential to transform business and society, and working with data will be essential to virtually every job in their organizations. On the other hand, these business and IT leaders have low confidence in their own knowledge about AI and the data their company possesses. They’re even more skeptical of their colleagues’ and their organization’s readiness to take advantage of the potential both in their troves of untapped data and in the power of AI. Make “data driven” a reality, and make sure your approach to data helps organize and surface it in ways that will let you tap all of its potential. Understand your data. Commit to bringing more of it out of the dark, out of the gray areas, to be a vital part of your decision making. Data, like money, is an asset. Like money, a business has a fundamental responsibility to keep track of it, and use it to best advantage."
  },
  "doc-807192c18974bf15cf16d4595821b14c": {
    "content": "Category: Data Infrastructure, Governance, Security\n\nTopic:  Data Governance\nOne common challenge with a data lake architecture is the lack of oversight on the contents of raw data stored in the data lake. Organizations need governance, semantic consistency, and access controls to avoid the pitfalls of creating a data swamp with no curation. Data catalogs play a crucial role in data governance by providing a centralized and organized repository of metadata and information about an organization's data assets. In active metadata management, metadata is not just stored and documented, it is continuously updated, enriched, and used in real-time to support various data management and data governance activities. The rich ecosystem of metadata connectors and dynamic discovery or crawling of data assets are key enablers for active metadata management. Data governance, focuses on the broader aspects of managing and governing data assets, including data quality, privacy, and compliance, while AI governance specifically addresses the ethical, transparent, and accountable use of AI technologies. These two governance areas often intersect and complement each other to ensure that AI systems are built on high-quality, compliant, and ethically managed data. \n\nTopic: Data Security\nData security primarily focuses on protecting data assets, including data at rest, data in transit, and data in use. It encompasses the confidentiality, integrity, and availability of data. Data masking, anonymization, tokenization, classification, and data privacy are important components of data security strategies aimed at protecting sensitive and confidential information. Data security encompasses a broader range of measures and practices aimed at protecting data assets throughout their lifecycle. AI security is a subset of data security that focuses specifically on the security of AI technologies, models, and algorithms. AI security deals with specific threats and risks associated with AI systems including machine learning models, algorithms, and AI applications. These threats may include adversarial attacks, model vulnerabilities, data poisoning, and model inversion attacks that target AI models and their operations. Data poisoning is a significant concern in AI/ML, especially when models are used in safety-critical applications like healthcare, finance, and autonomous systems.\n\nTopic: Data Infrastructure\nEdge computing moves data processing and analysis close to endpoints where data is generated, to deliver real-time responsiveness and reduce the cost associated with transferring large amounts of data. Edge computing can pre-process data, extract relevant information, and perform initial analytics locally. After this initial processing, summarized or relevant data can be sent to a data lake or edge network for further analysis, archival, and integration with historical data. Fog computing is a decentralized computing paradigm that extends cloud computing capabilities to the edge of the network, typically one or more hops away from the edge devices. It can be thought of as an intermediate layer between the edge devices and the centralized cloud. Organizations often require both real-time analytics at the edge and more extensive, historical analysis in a centralized data lake. Hybrid analytics solutions combine edge computing, fog computing with cloud or data lake-based analytics to provide a holistic view of data. It is important to conduct a thorough assessment of data lake, AI platform requirements and workloads to determine the appropriate cost effective infrastructure need for processing power. Emergence of hybrid cloud capabilities make it a valuable tool for organizations pursuing a comprehensive data lake strategy that spans multiple environments including edge, on-premise and multiple cloud service providers. Such capabilities include a cloud-centric container platform to run computationally intensive workloads by AI/ML applications utilizing CPU’s, GPU’s at scale. Most AI/ML workloads are cloud native in nature, deployed in containers and optimized to work with GPUs. They need to co-exist with other workloads on a shared infrastructure which results in a mix of virtual machines (VMs) and containers.\n\nOperationalizing Data Lakes\nWhile DevOps, DataOps and MLOps share a focus on automation, collaboration, and efficiency, they have distinct areas of emphasis. DevOps is primarily concerned with software development and deployment, while DataOps is focused on data-related processes and workflows. DataOps seeks to address challenges related to data integration, data quality, and data delivery, making it more tailored to the needs of data-driven organizations. MLOps is specifically focused on managing and operationalizing machine learning models, ensuring they perform well in production environments. By supporting interoperability of machine learning (ML) models, such as using the Open Neural Network Exchange (ONNX) format or PMML, ML models can be easily shared and deployed across different ML frameworks and inference engines.\nIntegrating DevOps, DataOps, and MLOps for a data lake requires a comprehensive strategy that promotes collaboration, automation, and efficient workflows across these disciplines. There is a need to effectively integrate DevOps, DataOps, and MLOps practices to create a cohesive and efficient data lake environment that supports data quality, data delivery, and machine learning model deployment in a streamlined and automated manner."
  },
  "doc-ad395b438c54466e0d7d405d6676ea94": {
    "content": "Category: Data Infrastructure, Governance, Security\nTopic: Data Mesh\n\nThe Data Mesh is one of three important emerging data architectures; the other two are Data Fabric and Data Lakehouse. Organisations need to clearly understand what each of them is, why they are important and how to implement them at scale, in a hybrid landscape. Modern Data Strategy will look at the organisation’s data sources, data consumption, analytics goals to determine the optimal architecture to maximize value of data. Data Mesh is a new way of creating and sharing ‘Data-as-a-Product’ by decentralizing data ownership and accountability. However, extensive research, thinking and decision making are required to define proper data domain strategy as a pre-requisite to Data Mesh implementation. Next critical business task is to define trustworthy and accurate data products that can be easily discovered and shared across organization\nData Mesh is ideally suited for organizations dealing with: \n1.\tConstant change in the topology of their data landscape \n2.\tProliferation of data sources and consumers \n3.\tDiversity in data transformation and processing needs \n4.\tThe need to respond to data-related change quickly If you have ongoing change and complexity in your data landscape, along with a proliferation of sources and consumers, and you are dissatisfied with the data and AI investment expended versus the results achieved, instituting a Data Mesh approach is seriously worth considering. \n\nData Mesh is an architectural and organizational paradigm shift for how companies work with and share data internally within their organization and/ or externally with their business partners. A key component of this paradigm shift is treating data as a product—allowing vertical teams, “domains,” to build and share data products horizontally across your company. The benefits of this layered approach can be enormous; each vertical team builds relevant and valuable data products to be used and combined with other domains’ data products. It also allows decentralized creativity, flexibility, and utilization of data products while empowering centralized discovery and federated governance. \nData Mesh can be thought of like an “app store,” except the apps are not apps at all, they are data products. And for data products to be “installed” and “run,” you need discovery, storage, compute, data definitions and documentation and, of course, governance and security. As an example, from the context of Healthcare and Life Sciences industry, “app store” can be a collection of data topics such as patient demographics, claims, clinical and real-world evidence data that help rapidly uncover patient/population level health insights, enable better patient/member experience, accelerate drug development, and more. e. Using our app store analogy again, you can build any app you want, but in order to monetize it, you must follow guidelines, publish to standards, and meet security controls before anyone can find or buy it.\nThe key components of Data Mesh are: \n1.\tDomain-Centric Ownership \nThe vertical teams, or the domains, that own the creation of the data products. They act as product managers and data engineers, owning their own data product roadmaps, pipelines, and data transformations, as well as documentation. \n\nFor organisations that have or are in the process of data consolidation on the data lake, there would be an existing data lake or data platform team that is responsible for ingestion of the data to the data lake. The Data Platform Team is often a separate team in larger organizations that is responsible for the underlying data platform and governance. This team also assists in developing standards and templates to ensure that the Domain Data Teams are following the best practices and are productive on the data platform. The Domain Data Team is the team that uses a self-serve platform to build and consume data products.\n\n2.\tSelf-Service Data Platform \nA distributed but interconnected set of compute, storage, tools and capabilities that avoids silos and enables distributed domain teams to build and exchange data products through ingestion, transformation, and provisioning of data. \n3.\tFederated Governance and Security\nHorizontal interoperability standards and policies, horizontal data governance policies, and vertical domain-specific governance policies. This is how the company can ensure data remains secure while still providing data product teams the freedom and power of decentralization. \n4.\tData-as-a-Product \nData as a Product must be easily discoverable, subscribable, and understandable through documentation. Data as product supports wider thinking about the F.A.I.R qualities: Findable, Accessible, Interoperable and Reusable. A platform allows domain teams to operate independently and easily share data products with each other.\n\nThe first component of Data Mesh is an organizational change; the other three components also involve some organization change (people, process aspects) on top of the technology changes. Data Mesh architecture requires a strong change management process for an effective roll out.\n\n\nSTEPS TO CREATE A DATA MESH ARCHITECTURE\n1.\tSelf-Service Data Platform \nDefine Data Platform where data domain owners can access the data in a secure manner. Depending on where the organization is in the maturity of the implementation of their data strategy, different options are available to facilitate access to the data aligned to the data domain. If the organization has majority of the data consolidated on the data lake in different data zones, cloud data warehouse platform such as Snowflake can be used as the self service data platform. Alternatively, if the organization has adopted delta lake as the storage file format, data domains can access the domain data by querying the delta lake files from the raw zone or enriched or published zone. Lakehouse Federation capabilities enable organizations to create a highly scalable and performant data mesh architecture with unified governance. Alternatively, tools such as Prophecy can be used as the Self-Serve Platform that retrieves the data from the data lakehouse. If the organization is in the early stage of consolidating their data on the data lake or have data sources corresponding to a data domain that do not yet have the data on the data lake, then data virtualization tools can be leveraged to access the data directly from the transactional system or the system of record. Tools such as Databricks Lakehouse Federation in Unity Catalog allow customers to discover, query, and govern data across all of their data platforms from within Databricks without moving or copying the data first to the data lakehouse. While transactional system performance can be a concern when querying data for analytics in a data mesh architecture, careful design including caching, query optimization, and the use of replicated data storage and scalable data processing technologies can help mitigate these concerns and ensure that both operational and analytical needs are met efficiently. Tools such as StarBurst provide a platform with a rich ecosystem of connectors to connect to various data systems without needing the data to be consolidated on the data lake and also include data modelling capabilities to model the data corresponding to the data domain.\n\nCompanies adopting a Data Mesh architecture must have an analytics engine capable of federating across these different data sources. Starburst is the analytics engine for the Data Mesh architecture, providing a single point of access to distributed data and empowering self-service analytics for each of the business domains. Starburst is built on open-source Trino, a distributed engine that can execute SQL queries against data stored in a range of databases and file systems. With Starburst and Trino, teams can lower the total cost of their infrastructure and analytics investments, prevent vendor lock-in, and use the existing tools that work for their business so that they can concentrate on enabling faster time-to-insights. Trino’s open technology means that integration with other open technologies such as data catalogs and data discovery tools is simpler and reduces the total cost of ownership of the self-service data platform.\n\n2.\tFederated Governance and Security\nNow that your data product teams have the autonomy of a self-service data platform, you must add access control and security. Complexity arises here because many data access control policies are not domain-specific and need to be enforced globally and consistently across domains, regardless of the data product. But this must be done in a way that also allows the data product teams to layer on additional domain specific controls. The following are key components of federated governance and security: 1. Ability to delegate and assign policy ownership to users globally or scoped more precisely to specific domain owners and their data products. 2. Ability to create a common taxonomy for how to describe and represent data through table and column tags and automatically tag said data. 3. Ability to author policies specific to your domain of control (global or domain-specific) without creating policy conflicts or disrupting existing policy. 4. Ability to detect and examine query activity against your data product(s).\n\nDistributed access control is central to the adoption of Data Mesh. Data Mesh distributes data ownership to domain owners, so in order to facilitate data access for Data Mesh, access controls must also be distributed. This is accomplished by enabling domains to be autonomous with respect to access controls. Autonomy can be achieved through the use of approaches such as Policy as Code and attribute-based access control (ABAC), where the policies are associated with the Data Product. Traditional access control solutions are much less suitable, as they are often static, role-based models that lead to data policy duplication, data copies, maintenance complexity, and eventual evolution toward centralisation. Migrating to a distributed access control model allows you to empower data owners to take control of their Data Products, while ensuring data is accessed only by the right people, for the right reasons, at the right time.\n\nHow do organisations ensure they are ready for distributed access control in the Data Mesh architecture? Step one is to separate policy from platform. Much like the separation of compute from storage has allowed Data Mesh architectures to grow, separating data policy from platform enables the flexibility and scalability that Data Mesh requires. This approach allows you to associate policies based on data domains and Data Products that scale. As more domains within an organisation adopt Data Mesh, the policy landscape becomes increasingly complex with regulations, data use agreements, and data sharing requirements. Policies should therefore be created with the data product, not with the platform. Step two is ensuring that organisations with a dynamic access control solution can avoid that policy management from growing out of control. As more Data Products are created, policy implementation must be flexible to ensure that hardcoded values, like roles or custom SQL logic, do not introduce unnecessary maintenance overhead. Modern data access platforms provide scalable, flexible, and easy-to-use ways to manage data access and governance. Finally, data use and data sharing agreements are now becoming the standard in Data Mesh architectures. Every Data Mesh architecture will need the ability to assign data use agreements to Data Products. This ensures that Data Products are used appropriately under Zero Trust or on an as-needed basis, and allows organisations to future-proof their architecture from current and forthcoming requirements that must have purpose-based or use agreements in place.\n\nTools such as Immuta provides data teams one universal platform to control access to analytical data sets in the cloud. Immuta can automate access to data by discovering, protecting, and monitoring data.\n\n3.\tData As A Product \nThe final critical component of Data Mesh is data as a product. It is important to understand what a data product is. Different types of data products can exist in a Data Mesh including: \na.\tPhysical data products. Physical data products are persisted datasets that have been produced, stored, and published in a data marketplace to make them available for consumption.\nb.\tVirtual data products. Virtual data products are virtual views that integrate data from one or more underlying data sources (including ready-made physical data products) on-demand, on a timer driven basis or on a continuous basis. They can also be materialised for performance. Again, they can be published in a data marketplace for people to discover, query and use.\nc.\tStored queries. Stored queries are typically SQL queries that can be published as services and invoked on demand e.g., via a REST or GraphQL API. When executed, the stored query will then produce a data product and serve it. Stored queries can integrate data from one or more data sources including other data products.\nd.\tAnalytical Products. BI reports, dashboards, predictive machine learning models, prescriptive machine learning models (e.g., recommendation engines), chat bots, automation bots are examples of analytical data products.\n\nOnce data product is created, it is possible for data domain teams to consume data products in another pipeline and integrate this data with other data domains to produce a new data product. This new data product can itself then be added to the Data Mesh. In that sense Data Mesh is a kind of bootstrapping capability where more and more data products are incrementally built over time and made available for others to consume and use. Therefore, as more data products become available, the time taken to deliver insights should get shorter because each new project doesn’t have to start from scratch to create data.\n\nThe data should be easy to discover, self-describing and accessible for use. Just like an app store, data products must be easily discoverable, subscribable, and understandable and provide a data portal user interface or a data sharing platform or marketplace to support data as a product. Key components of this data portal are that: \nA.\tData products are searchable by tag, name, or documentation content \nB.\tData product owners can fully document the data products to include documenting individual columns \nC.\tData products can be hidden from consumers based on policy \nD.\tConsumers that don’t meet policy to gain access to a data product can be allowed access through manual just-in-time approvals (if prescribed in the policy).\nDatabricks Delta tables can also used as a data product that can be published and consumed, using multi-model access. The combination of Unity Catalog, Delta Sharing and Databricks Marketplace helps address the requirement of implementing a Data Mesh. Tools such as Prophecy provide a Self-Serve Platform built on top of the data lakehouse to build, publish and find data products. Data as a Product can also be published to a marketplace for data monetization. Data can be made discoverable to publishing the data in a data catalog."
  },
  "doc-80bee268b85178466f8a4508a60e238e": {
    "content": "Category: Data Infrastructure, Governance, Security\nAdditional Category: Organise & Prepara\nTopic: Data Quality\n\nWhat Is Data Quality? \nData quality refers to the degree of accuracy, consistency, completeness, reliability, and relevance of the data collected, stored, and used within an organization or a specific context. High-quality data is essential for making well-informed decisions, performing accurate analyses, and developing effective strategies. Data quality can be influenced by various factors, such as data collection methods, data entry processes, data storage, and data integration. Maintaining high data quality is crucial for organizations to gain valuable insights, make informed decisions, and achieve their goals.\n\nWhy Is Data Quality Important?\nHere are several reasons data quality is critical for organizations:\n\n1. Informed decision making: Low-quality data can result in incomplete or incorrect information, which negatively affects an organization’s decision-making process. With access to accurate and dependable data, business leaders can make informed decisions that promote growth and profitability.\n2. Operational efficiency: Data quality has a direct influence on operational efficiency by providing all departments with the accurate information needed for everyday tasks, including inventory management and order processing. Improved data quality leads to reduced errors in these processes and increases productivity.\n3. Customer satisfaction: Inaccurate customer records can make it more difficult to provide quality service to customers. Maintaining high-quality customer databases is crucial for improving satisfaction among existing clients.\n4. Revenue opportunities: Data quality directly affects an organization’s bottom line by enabling more effective marketing strategies based on precise customer segmentation and targeting. By using high-quality data to create personalized offers for specific customer segments, companies can better convert leads into sales and improve the ROI of marketing campaigns.\n\n\nData Quality vs. Data Integrity\nData integrity concentrates on maintaining consistent data across systems while preventing unauthorized changes or corruption of information during storage or transmission. The primary focus of data integrity is protecting data from any unintentional or malicious modifications, whether it is in storage or transit.\nKey differences between data quality and data integrity include:\n1. Objective: While both concepts aim to improve overall trustworthiness in an organization’s information assets, their primary focus differs. Data quality targets specific attributes of individual records, while data integrity ensures reliability throughout the entire data lifecycle, including creation, update, deletion, storage, and transmission.\n2. Methods: Enhancing data quality might involve cleansing, standardizing, enriching, or validating data elements, while preserving data integrity necessitates robust access controls, encryption measures, and backup/recovery strategies.\n3. Scope: Data quality primarily deals with dataset content, while data integrity is more concerned with the overall system architecture and processes that ensure consistency across different platforms or applications.\n\nWhat are the six key elements that contribute to data quality?\n1. Completeness: Is all the necessary data included in the dataset?  Completeness concerns whether a dataset contains all necessary records, without missing values or gaps. A complete dataset allows for more comprehensive analysis and decision-making. To improve the completeness, you can use techniques like imputing missing values, merging multiple information sources, or utilizing external reference datasets.\n2. Consistency: Is the data consistent between sources and over time? Consistency measures the extent to which data values are coherent and compatible across different datasets or systems. Incorrect data can cause wrong conclusions and confusion among different users who rely on the information to make decisions. To improve consistency, you can implement data standardization techniques, such as using consistent naming conventions, formats, and units of measurement.\n3. Validity: Does the data conform to the required format or structure? \n4. Uniqueness: Are there any duplicates in the dataset? Uniqueness refers to the absence of duplicate records in a dataset. Duplicate entries can skew analysis by over-representing specific data points or trends. The primary action taken to improve the uniqueness of a dataset is to identify and remove duplicates. You can use automated deduplication tools to identify and eliminate redundant records from your database.\n5. Accuracy: How correctly does the data reflect what occurred in the real world? Accuracy refers to the extent to which data accurately represents real-world values or events. Ensuring accuracy involves identifying and correcting errors in your dataset, such as incorrect entries or misrepresentations. One way to improve accuracy is by implementing data validation rules, which help prevent inaccurate information from entering your system.\n6. Timeliness: Is the data available when it is expected and needed? Timeliness ensure that your data is up-to-date and relevant when used for analysis or decision-making purposes. Outdated information can lead to incorrect conclusions, so maintaining up-to-date datasets is essential. Techniques like incremental updates, scheduled refreshes, or real-time streaming can help keep datasets current.\n\nTo manage data quality effectively, we recommend focusing your efforts on the following five areas:\n1.\tOversight: Your data is a strategic asset for your organization, and it needs to be managed as such. Key stakeholders that depend on the quality of the data should meet on a regular basis to drive organizational accountability for maintaining data quality. Data governance initiatives may include both employee education on the importance of data quality, as well as the introduction of data policies and procedures to help avoid downstream data problems. \n2.\tValidate: Before the data is enriched or published to the data lake, it is preferable to verify its quality using data profiling and cleansing tools. It might also be helpful to establish data standards to make it easier to spot egregious issues. \n3.\tPrevent: A key preemptive measure to keep data clean is to create documentation around the data attributes or create a data dictionary. For example, it can be helpful to maintain a metric/dimension glossary that users can reference and establish naming conventions for datasets and their various sub-elements to keep them consistent over time. \n4.\tMonitor: It’s helpful to keep a watchful eye on the incoming data and setting up alerts to monitor key metrics or datasets. \n5.\tAudit & Fix: Once an issue has been identified, you need a systematic process for resolving data problems. Rather than introducing a Band-Aid solution, we recommend tracing the issue back to its root cause to avoid future issues that might appear due to a temporary fix.\n\nStrategies for Improving Data Quality\n1. Establish Data Governance Policies\nCreating data governance policies ensures uniformity in handling and managing data throughout your organization. These policies should outline roles, responsibilities, standards, and processes related to data management. Implementing clear guidelines on collecting, storing, processing, and sharing information within the company can, over time, significantly improve overall data quality.\n\n2. Offer Data Quality Training\nProviding training programs focused on data quality management equips employees with the knowledge and skills needed to handle information responsibly. Regular workshops or seminars, covering topics like data collection practices or error detection techniques, will empower team members to contribute to high data quality standards.\n\n3. Keep Documentation Accurate and Up-to-Date\nMaintaining current documentation about your data sources, processes, and systems helps users understand the context of the information they are working with. This documentation should include details about data lineage (how it was collected), transformations applied to it, and any assumptions made during analysis. Accurate documentation can help prevent misunderstandings that may lead to incorrect insights.\n\n4. Implement Data Validation Techniques\nData validation techniques are essential to guarantee accurate input into your systems. Introducing checks like format validation (e.g., validating that email addresses are correct), range constraints (e.g., age limits), or referential integrity rules (e.g., foreign key constraints) helps prevent incorrect or inconsistent values from entering your databases.\n\n5. Implement Feedback Loops\nFeedback loops involve gathering input from end-users regarding potential inaccuracies in datasets or reporting outputs. Fostering a culture of open communication around possible errors allows organizations to identify problems quickly and proactively implement necessary changes, rather than reacting after the fact when consequences may already have occurred.\n\n6. Use Data Cleansing Tools\nData cleansing tools are designed to automatically identify errors in datasets by comparing them against predefined rules or patterns. These tools can also be used for tasks like removing duplicates from records or normalizing values according to specific criteria (e.g., capitalization). Regularly using these tools ensures that your systems store only high-quality information.\n\n7. Monitor Data Quality Metrics\nMeasuring data quality metrics, such as completeness, accuracy, consistency, timeliness, or uniqueness, is crucial for identifying areas where improvements can be made. Regularly monitoring these metrics enables you to detect issues early on and take corrective actions before they affect business operations.\n\nWhat is data Obervability?\nData observability, often referred to as data observability or data monitoring, is a practice in data management and data engineering that focuses on ensuring the reliability, quality, and performance of data pipelines and data-related processes. It involves the continuous monitoring, tracking, and analysis of data as it flows through various stages of data processing, storage, and transformation. The primary goal of data observability is to maintain data integrity, detect anomalies, and troubleshoot issues in real-time or near-real-time."
  },
  "doc-c01239c5ab57ca0131d8c8a0350f7287": {
    "content": "Categories: Analyse, Consume, Infrastructure\nTopic: Data Science Platform\n\nTypes of Data Science Platforms\nThe data science platform landscape can be overwhelming. There are dozens of products describing themselves using similar language despite addressing different problems for different types of users. \nWe can divide the types of Data Science Platforms into 3 categories. They are: \n1. Automation Tools\nThese tools provide MLOps to help engineers to automate repetitive tasks in data science, including training models, selecting algorithms, and more. These solutions are targeted primarily at non-expert coders or data scientists interested in shortcutting tedious steps and repetitive steps. They help spread data science work by getting non-expert data scientists into the model-building process, offering drag-and-drop interfaces. \n2. Data Science Platforms\nProprietary tools such as Domo, Dataiku support a lot of use cases, including data science and model building using drag and drop UI or low code/no -code. They provide both drag-and-drop and code interfaces and have a stronghold in big companies and may even offer unique capabilities or algorithms. While these solutions offer a great breadth of functionality, the licensing cost could be prohibitive for small organization.  Most of the data science platforms have MLOps integrated in the AI/ML pipelines. There are open source alternatives such as KNIME.\n3. Code-first Data Science Platforms\nCode-first Data Science Platforms target data scientists and coders who use statistical programming languages and spend their days in IDEs like Jupyter and Colab, leveraging a mix of open-source and Machine Learning packages and tools to develop sophisticated models. These data scientists require the flexibility to use a constantly evolving software and hardware stack to optimize each step of their model lifecycle. These code-first data science platforms orchestrate the necessary infrastructure to accelerate power users' workflows and create a system of record for organizations with hundreds or thousands of models.\n\nNeed for a Data Science Platform\n1. To Enable Better Teamwork with Data Scientists\nIf the data scientists are solving the same problem in several ways and working separately, productivity will decrease as it will not deliver effective value to the organization. \nIf the whole team of data scientists works on a unified and single platform, where they are provided with the required tools, it ensures that all the contributions of the data scientists, i.e., data models, data visualizations, and code libraries, exist in a single shared reachable location. This helps data scientists to reuse the code, facilitate better discussion around research projects,  \n2. Help Minimalize Engineering Effort\nWith data science platforms, data scientists get help in moving analytical models into production. A data science platform makes sure that the data models are accessible behind an API so that the data scientists do not have to depend much on engineering efforts. \nIt will decrease the additional engineering effort or DevOps. For instance, if a company wants to build a product recommendation engine, then the data scientist will require the efforts of a software engineer for testing, refining, and integrating the data model before the users start seeing the product recommendations on the basis of their behavior \n3. Help to Offload a Number of Low Value Tasks\nData scientists can cut off the burden of menial tasks such as reproducing past results and configuring new environments for non-technical users for every project, as these tasks can be efficiently handled with data science platforms. \n4. Facilitate Faster Research and Experimentation by Collaboration\nWhenever there is a new person in the data science team, the employee can start working exactly from the point where the old employee left, as it is easier to restore the work through the unified platform. Data scientists do not have to deal with extra data management tasks, as data science platforms allow people to see what and how others are working on."
  },
  "doc-10e4f491e491476eafe20e73bf5100d2": {
    "content": "Categories: Analyse, Consume, Infrastructure\nTopic: Data Sharing\n\nWhat Is Data Sharing and Why Is It Important?\nData sharing is the ability to make the same data available to one or many stakeholders — both external and internal. Nowadays, the ever-growing amount of data has become a strategic asset for any company. Data sharing — within your organization or externally — is an enabling technology for data commercialization and enhanced analysis. Sharing data as well as consuming data from external sources allow companies to collaborate with partners, establish new partnerships and generate new revenue streams with data monetization. Data sharing can deliver benefits to business groups across the enterprise. For those business groups, data sharing can get them access to data needed to make critical decisions. This includes but is not limited to roles such as the data analyst, data scientist and data engineer.\n\nWhat are the key benefits of data sharing?\nThe benefits of data sharing include: \n\nAbility to generate new revenue streams: With data sharing, organizations can generate new revenue streams by offering data products or data services to their end consumers. \n\nGreater collaboration with existing partners: In today’s hyper connected digital economy, no organization can advance their business objectives without partnerships. Data sharing helps cement existing partnerships and establish new ones. \n\nEase of producing new products, services or business models: Product teams can leverage both first-party data and third-party data to refine their products and services and expand their product/ service catalog.\n\nGreater efficiency of internal operations: Teams across the organization can meet their business goals far more quickly when they don’t have to spend time figuring out how to free data from silos. When teams have access to live data, there’s no lag time between the need for data and the connection with the appropriate data source.\n\nWhat are the Conventional Methods of Data Sharing and Their Challenges\nSharing data across different platforms, companies and clouds is no easy task. In the past, organizations have hesitated to share data more freely because of the perceived lack of secure technology, their competitive concerns and the cost of implementing data-sharing solutions. Even for companies that have the budget to implement data-sharing technology, many of the current approaches can’t keep up with today’s requirements for open-format, multicloud, high-performance solutions. Most data-sharing solutions are tied to a single vendor, which creates friction for data providers and data consumers who use noncompatible platforms. Over the past 30 years, data-sharing solutions have come in three forms: legacy and homegrown solutions, cloud object storage, and closed source commercial solutions. \n\nTHE PITFALLS OF USING APIS FOR DATA SHARING AND HOW TO AVOID THEM\nIf you are just starting out in your data monetization journey, you might be tempted to develop APIs as a way of sharing data. Although APIs are a great way to connect different systems and automate processes, they have a series of additional challenges when they are used for data exchange, including: \n1.\tRequiring in-house expertise to develop and maintain them \n2.\tRequiring recurring effort and costs to develop and maintain them \n3.\tLimiting the volume of accessible data \n4.\tRequiring data consumers to learn how to use the API \n5.\tLimiting the types of questions the data buyer can ask against the data \n6.\tCausing performance and quality issues that are difficult to resolve\n\nLegacy and homegrown solutions: Many companies have built homegrown data-sharing solutions based on legacy technologies such as email, (S)FTP or APIs\n\nProprietary vendor solutions: Commercial data-sharing solutions such as Snowflake Data Sharing are a popular option among companies that don’t want to devote the time and resources to building an in-house solution yet also want more control than what cloud object storage can offer. Commercial data-sharing solutions also offer managed packaged solutions in marketplace to allow ease of sharing.\nCloud object storage Object storage is considered a good fit for the cloud because it is elastic and it can more easily scale into multiple petabytes to support unlimited data growth. The big three cloud providers all offer object storage services (AWS S3, Azure Blob, Google Cloud Storage) that are cheap, scalable and extremely reliable. An interesting feature of cloud object storage is the ability to generate signed URLs, which grant time-limited permission to download objects. Anyone who receives the presigned URL can then access the specified objects, making this a convenient way to share data.\n\nWhat are the use cases for Data Sharing?\nData sharing with partners or suppliers (B2B) \nMany companies now strive to share data with partners and suppliers as similarly as they share it across their own organizations. For example, retailers and their suppliers continue to work more closely together as they seek to keep their products moving in an era of ever-changing consumer tastes. Retailers can keep suppliers posted by sharing sales data by SKU in real time, while suppliers can share real-time inventory data with retailers so they know what to expect. Scientific research organizations can make their data available to pharmaceutical companies engaged in drug discovery. Public safety agencies can provide real-time public data feeds of environmental data, such as climate change statistics or updates on potential volcanic eruptions. \n\nInternal lines of business (LOBs) sharing \nWithin any company, different departments, lines of business and subsidiaries seek to share data so that everyone can make decisions based on a complete view of the current business reality. For example, finance and HR departments need to share data as they analyze the true costs of each employee. Marketing and sales teams need a common view of data as they seek to determine the effectiveness of recent marketing campaigns. And different subsidiaries of the same company need a unified view of the health of the business. Removing data silos — which are often established for the important purpose of preventing unauthorized access to data — is critical for digital transformation initiatives and maximizing business value of data. \n\nData Monetization\nCompanies across industries are commercializing data, and this segment continues to grow across industries. Large multinational organizations have formed exclusively to monetize data, while other organizations are looking for ways to monetize their data and generate additional revenue streams. Examples of these companies can range from an agency with an identity graph to a telecommunication company with proprietary 5G data or to retailers that have a unique ability to combine online and offline data. Data vendors are growing in importance as companies realize they need external data for better decision-making.\n\nFOUR STEPS TO START YOUR DATA MONETIZATION JOURNEY\nSTEP 1: IDENTIFY YOUR DATA ASSETS \nThe first order of business is to define the inventory of potentially shareable data. Information that would disclose trade secrets, otherwise jeopardize competitiveness, or run afoul of legal protections and privacy policies obviously won’t make the cut. That still leaves a wealth of possible inventory, including Operational data that includes Transaction records and sensor logs,  Commercial data that includes Industry developments, sentiment, and prices, Marketing data that includes Aggregated or de-identified customer information, preferences, web traffic,Behavioral data that includes Data captured in digital and physical environments and SaaS data that Serves the customer data that’s created by your app back to your clients Alternatively, you may have analytical information that incorporates open-source data such as social network posts or government statistics. And, last but not least, every organization has “dark data,” information that is collected as part of regular business activities but is not used or analyzed. When mined and combined with other signals, this data can provide interesting insights and be a valuable component of monetization strategies.\nCOMMON TYPES OF DATA OFFERINGS The type of data offering may determine how to charge for it and how much to charge. Here are five of the most common types of data or data services you can monetize: \n1.\tRaw Data in its original form that has not been processed, analyzed, or transformed \n2.\tPackaged data product or Ready-to-consume data that includes aggregation and requires little or no analysis/transformation\n3.\tData analysis or insights using Dashboards, metrics, and indices \n4.\tData enhancement as a service that augments customer data with additional insights \n5.\tData trade or exchange or Using your data to pay for data access\nAs you identify the types of data you own, you will have to decide whether giving access to the raw data is of value to customers or if the data needs to be combined or enhanced with additional data sets. For example, a retailer’s store-level data is valuable to suppliers as is, but they might be willing to pay a premium if that data were enriched or scored with weather and demographics data sets.\nDepending on the types of customers you have, adding a layer of analytics to the data—that is, creating specific dashboards and reports—might be exactly what they need to make better decisions, especially if they lack the expertise or resources to perform the data analysis themselves. The more insights and enrichments you add to the data, either by incorporating more data sets into the original source or through the creation of prebuilt analyses, the higher its potential value. Adding insights to a data set increases its value.\n\nSTEP 2: CHOOSE A PRICING STRATEGY \nDifferent methodologies exist for pricing your data, each with its own benefits. Two of the most common ways of looking at how to price your data products are cost pricing and value pricing. Cost pricing Cost pricing involves understanding your costs for data collection, storage, preparation, transformation, and sharing so you can add a percentage margin as you price your data above your costs. You should consider the following: \n1.\tCost of data sourcing: The time and effort taken to select and extract data sets, \n2.\tCost of data packaging: The time and effort related to preparing the data for consumption and any related augmentation or enrichment done to the data, \n3.\tCost of data sharing: The time, effort, and other costs associated with copying, storing, and transferring data to the consumer.\nWhat if your goal is not to maximize data revenue, but rather to use the offering as a customer acquisition tool? In that case, you might price your data at or below cost as a loss leader, or even give some of it away for free. The size of the discount might then depend on the value of the new business sought and the expected conversion rate of prospects into clients.\nValue pricing \nValue pricing involves looking at your data from a customer’s perspective and identifying the value it will bring. With this pricing strategy, consider the following: \n1.\tUniqueness: Is this data unique in any way or form? \n2.\tAccess restrictions: Is the data difficult for customers to access? Are there specific barriers (physical or regional data locality laws or otherwise) preventing customers from obtaining the data themselves in some other way? \n3.\tTechnology and expertise: Is aggregating or using this data technically difficult? Does it require specific expertise not found in many companies? \n4.\tMarket alternatives: Are there other companies already providing similar data sets? Where would customers have to go in order to acquire similar data sets and at what cost? \n5.\tAnalysis and insights: Is the analysis of the data time-consuming and costly? Are customers already paying (either in consulting fees or in additional internal resources) to analyze this type of data? \n6.\tBusiness value: Most importantly, will this data help companies improve their business operations, performance, or customer satisfaction? Could it help them develop better products or services\nPackaging \nThe final element in the pricing analysis is what we call packaging. Determining costs and value is helpful in establishing different pricing tiers, or packages. The traditional “good, better, best” framework also applies to your data products, with the following elements to consider: \n1.\tTimeliness: How fresh is the data? Should there be options for acquiring new versus historical data sets? What about updates or corrections to previously delivered data? \n2.\tUpdate frequency: How often would you need to update the data? Would customers be willing to pay for more frequent updates? \n3.\tScope: How broad is the data product and is there potential to offer segmentation or various “cuts” of a set of data by separately packaging and pricing different intersections of tables, rows, and columns? Some customers may be willing to pay a premium for larger data sets while others might be interested only in narrower data sets. \n4.\tDistribution breadth: Will you offer data products to anyone who wants to buy them, or only for certain types of buyers or use cases? Will you limit the number of parties that can buy each sleeve of data, to increase scarcity and therefore positively affect price? \n5.\tAdditional services: Would adding access to analytics, prebuilt dashboards, or preconfigured schema and chart metadata for the most commonly used visualization tools make the data more attractive?\nA tiered pricing plan can help attract new users by offering data access-only plans at lower costs, while ensuring that your existing customers get the data and services they need at a cost that best fits their needs and budget. You’ll also need to decide whether to sell data by the set or by subscription, perhaps monthly or annually, or if you want to charge based on usage.\nConsider a freemium structure featuring limited teaser access for new leads, a charge for standard access, and premium fees for additional service features. Let freemium data be broad in terms of coverage scope (all geographies, for example), but limit the number of data columns, or raise the level of aggregation to leave freemium users “wanting more.”\n\nSTEP 3: SELECT A DISTRIBUTION CHANNEL \nData sellers now have a large and often bewildering array of choices for distributing data to data buyers, each with its advantages and drawbacks. Traditional methods include: \n1. Doing a direct data transfer (for example via SFTP or Amazon S3) \n2. Using a third-party data broker \n3. Using a data marketplace \n4. Use an open protocol over HTTPS such as delta sharing\n\nA direct data transfer to clients cuts out intermediaries and gives you more control over the final product. However, you do all the work, often with standards such as FTP and APIs, which have multiple disadvantages when it comes to storage and ETL costs, security vulnerabilities, service costs, and the potential for latency that can affect customer experience. Such solution are point to point solutions.\n\nA data broker such as revelate can help market your data and will sometimes also control pricing. But you’ll miss out on forging direct relationships with the ultimate users of your data, and you may not have the ability to choose who sees the data, or get a sense of how they are using it. Moreover, if you need to update the data on a regular basis, every engagement with a less sophisticated data broker may be like the first, requiring all the data transformation and loading you did the first time. \n\nTraditional data marketplaces also promise to help with client acquisition and pricing plans. But they offer limited opportunities for promotion and incomplete control over the presentation, in addition to the usual file transfer and update hassles. API based data marketplaces require both the buyer and seller to code to a bespoke API, and then maintain, troubleshoot, and update that code over time. Traditional distribution channels move data from point A to point B, often with a couple of stops in between. This means they all have a common problem: when data travels, it becomes vulnerable to corruption, loss, theft, latency, and obsolescence. \n\nWhat is Delta Sharing? Delta Sharing provides an open solution to securely share live data from your lakehouse to any computing platform. Recipients don’t have to be on the Databricks platform or on the same cloud or a cloud at all. Data providers can share live data without replicating it or moving it to another system. Recipients benefit from always having access to the latest version of data and can quickly query shared data using tools of their choice for BI, analytics and machine learning, reducing time-to-value.\nData providers can centrally manage, govern, audit and track usage of the shared data on one platform. \n\nSTEP 4: CHOOSE A DATA SHARING SOLUTION \nBy leveraging solution such as Deltashare open protocol or vendor proprietary Snowflake Secure Data Sharing capabilities, data sellers can easily publish a variety of data products, which then become immediately available for use or purchase. This has multiple benefits for both data sellers and data buyers. \nSnowflake’s Collaboration technology enables organizations to share data directly with their customers, suppliers, and business partners, without actually moving it. The data remains fully encrypted and stays put in the data seller’s Snowflake account; there are no duplicate data sets held by the buyer to chase down if regulations or relationships change, and data access is fully revocable at any time. The data is updated in near real time, not just whenever the IT schedules a refresh job. The data seller retains real-time, fine-grained control and determines who has access rights and can change or revoke them at any time.\n\nSnowflake Marketplace offers a more sustainable approach to providing data access to a broad audience. As a data seller, you can offer your data product under your own guidelines and update schedule—as a free offering or a commercial offering with your preferred pricing model. Anyone can find, try, and buy data products and services on Snowflake Marketplace with minimum effort and maximum efficiency.\n\nDelta Sharing is natively integrated with Unity Catalog, enabling organizations to centrally manage and audit shared data across organizations and confidently share data assets while meeting security and compliance needs. With Delta Sharing, organizations can easily share existing large scale data sets based on the open source formats Apache Parquet and Delta Lake without moving data. Teams gain the flexibility to query, visualize, transform, ingest or enrich shared data with their tools of choice.\n\nDatabricks designed Delta Sharing with five goals in mind: \n1.\tProvide an open cross-platform sharing solution \n2.\tShare live data without copying it to another system \n3.\tSupport a wide range of clients such as Power BI, Tableau, Apache Spark™, pandas and Java, and provide flexibility to consume data using the tools of choice for BI, machine learning and AI use cases \n4.\tProvide strong security, auditing and governance \n5.\tScale to massive structured data sets and also allow sharing of unstructured data and future data derivatives such as ML models, dashboards and notebooks, in addition to tabular data"
  },
  "doc-563573fa4d402d27e07c3ec12c30a812": {
    "content": "Category: Data Infrastructure, Governance, Security\nTopic: DataOps\n\n\nDataOps presents organizations with a set of principles that when correctly implemented, help to solve many of the challenges data, business and operations teams encounter. While the benefits of DataOps are clear, the best implementation path can be unclear. \n\nDataOps describes a set of tools and processes that, improve the speed and agility of developing data and analytics solutions while improving overall data quality. Inspired by DevOps, DataOps uses a set of principles and best practices in order to achieve its objectives: \n1.\tAchieving high data quality with continuous error detection \n2.\tBuilding transparency by monitoring and measuring results \n3.\tStreamlining data ingestion through process automation \n4.\tImproving collaboration between delivery and business teams \n5.\tDelivering with shorter release cycles\n\nTo understand DataOps, we need to start by understanding the concept in context with other similar methodologies. DataOps shares many of the objectives and practices of MLOps and DevOps – in fact, many organizations aspire to leverage multiple Ops cohesively to benefit from the natural synergies.\n\nOur approach is measuring and evaluating the efforts teams are spending on performing recurring data tasks and investigating data issues.\nThe data or ops team is consumed with data promotions or releasing data engineering features. The team may also be slowed down on other repeatable activities which are part of the data engineering workflow including but not limited to creating test data and retrieving real production data to get tasks started. On the business side, the time spent by the data team for each feature release is perceived as a black box with limited perceived value, serving only to delay attainment of business objectives.\nReducing Data Tasks: Your team will benefit greatly from automating sections of your data pipeline as well as reducing friction introduced by recurring data engineering tasks.\nReducing Data Issues: The business consumers of the analytics outputs are frequently asking the team to investigate either real or perceived data issues. This can bog down either ops or data teams, forcing them to investigate high priority issues which can be critical for decision making or sensitive downstream processes. End-users view the data as unrecognizable and potentially loose trust in the analytics. Your team will benefit substantially from statistical structural data validation of both the input source data and output analytics data. In this scenario, we recommend focusing on one of the dimensions to bring immediate value to the most impactful pain points. \n\nIntroducing statistical and structural testing doesn’t have to be overly complicated or engineered. Both statistical and structural data testing can be done by simple queries and scripts. When integrated in data pipelines it provides continuous validation in ongoing delivery, and achieves the following objectives: \n1.\tImprove transparency between data and business teams by defining pipeline input and output expectations \n2.\tDetect potential data anomalies before production release to enable proactive investigation\n3.\tIncrease data pipeline reliability by validating data sources are in the correct formats and within the expected range of values \n4.\tReduce breaks and errors by rejecting incompatible and erroneous data\n\nWe can continue to improve upon the data pipeline by implementing a quantitative and statistical evaluation to assess whether the results of our data transformation or aggregation output meets consumer expectations. The data pipeline workflow can be engineered to notify the Ops team where a data load doesn’t meet these threshold criteria (tip: don’t send notifications for regular, successful events to avoid notification fatigue). Naturally, the above rules we’ve defined will become part of a regular review cycle to ensure they are kept in alignment with end-user expectations and expanded as new pipeline features are developed.\nIn addition to the rule and threshold-based validation strategy, implementing a statistical validation of data provides significant benefits.\n\nThe Apache incubator, open-source projects and DataOps focused platforms are advancing the tool and technology landscape by providing continuous integration and development features for data projects. Data processing workflow & orchestrations as code, native source control integration for data structures and automation of data promotion processes are all contributing to this advancement. Existing progress is also encouraging new approaches to developing data pipelines often adopted by and integrated into existing products. Ultimately, the industry is evolving to achieve many of the desired outcomes listed are also objectives of DevOps. There are however substantial differences introduced with data centric applications. \n1.\tVisibility & Sensitivity \nA large portion of data analytics is used by systems or humans as input into varying levels of decision making. It can be argued that the use of data analytics for decision making purposes, inherently makes these types of solutions intrinsically sensitive to data issues. In many cases, the data validation process is partially composed of a subjective evaluation to determine if the data falls into a feel-good range. High volume data projects make this type of validation exercise significantly challenging. \n2.\tApplication State \nMost data analytics applications (as a whole) can be classified as stateful, since the output typically reflects a set of results for a specific context for a moment in time. Consequently, the change of state caused by an ongoing data refresh (sometimes happening on a frequent basis in production), or new feature release, has the potential to negatively impact the validity of the data. The results presented to the consumer could be dramatically different over the course of days, minutes or even seconds. Volatility in the results presented to a user in support of critical business decisions can significantly diminish their trust in the solution. \n3.\tData Engineering Workflow \nDataOps pipeline typically consists of several stages that collectively manage and streamline the end-to-end data operations process. These stages help ensure that data is efficiently and reliably delivered from source systems to its destination, whether for analytics, reporting, or other purposes. While the specific stages may vary depending on the organization and its data needs, the following are common stages in a DataOps pipeline: data extraction, data transformation, data ingestion, data quality checks and validation, data storage, data cataloging and metadata management, data access and security, data consumption, data monitoring and alerts, data versioning, testing, deployment automation.\n4.\tQuantitative Testing Benefits\nA larger proportion of the output from data analytics applications is numerical. Using a statistical approach to validate the application offers greater value (in most cases) than in software development projects."
  },
  "doc-6d01a52813f78b0f2def82ab1b850b86": {
    "content": "Category: Analyse\nTopic: NLP\n\n\nNLP is a branch of machine learning and AI which deals with human language, and more specifically with bridging the gap between human communication and computer understanding. Its practical applications span from topic extraction from documents, to sentiment analysis of clients putting reviews in social media, to getting insights about the needs and the struggles of people calling customer support services, or even going as far as building near human conversational agents to offload these call centers, for instance. NLP sounds like a very niche thing, but it’s actually incredibly prevalent. You’ve probably encountered a natural language processing system in your day to day life without realizing it. Some common subfields of NLP are: \n1.\tQuestion answering (search engines) \n2.\tSpeech recognition (Siri, Alexa) \n3.\tMachine translation - translating from one language to another (Google Translate) \n4.\tInformation extraction - pulling relevant details from unstructured and/or structured data (like important info from health records, relevant news that could impact a trade for a trading algorithm, etc.) \n5.\tSentiment analysis - detecting the attitude (positive, negative, neutral) of a piece of text (used by businesses on their social media comments or for customer service, etc.) \n\nHOW DOES NLP WORK?\n1. PRE-PROCESSING DATA The data must be cleaned and annotated (labeled) so that it can be processed by an algorithm. It’s also worth noting that newer techniques can leverage non-labelled data in pre-training models which would then be trained or fine tuned on labeled data (read more about this in the next section). Cleaning usually involves deconstructing the data into words or chunks of words (tokenization), removing parts of speech without any inherent meaning (like stop words such as a, the, an), making the data more uniform (like changing all words to lowercase), and grouping words into predefined categories such as the names of persons (entity extraction). All of this can be done using the spaCy library in Python. Annotation boils down to examining surrounding words and using language rules or statistics to tag parts of speech (similar to how we would use context clues to guess the meaning of a word). \n2. VECTORIZATION \nAfter preprocessing, the text data is transformed into numerical data, since machine learning models can only handle numerical input. Traditionally, the two main vectorization techniques that have been used most widely are Count Vectorization and Term Frequency-Inverse Document Frequency (TF-IDF). Count Vectorization involves counting the number of appearances of each word in a document or document section (i.e distinct text such as an article, book, a paragraph, etc.). The TF-IDF approach takes the logarithmic function of the size of the set of documents, and in how many documents a word appears. This is then multiplied by the term frequency to get a score. If the TF-IDF score is high, it means that it is good at discriminating between documents. This can be very useful, unlike Count Vectorization which only counts how many times a word occurs. Finally, a third technique called word embedding has nowadays become the dominant approach to vectorization. Embedding is a type of word representation that allows words with similar meaning to have a similar representation by mapping them to vectors of real numbers. Unlike older methods, word embeddings are able to represent implicit relationships between words that are useful when training on data that can benefit from contextual information.\n3.TESTING \nOnce a baseline has been created (the “rough draft” NLP model), its prediction accuracy is tested using a test subset. The model is built using the training subset and then tested on the testing subset to see if the model is generalizable-- we don’t want a model that only gives accurate predictions for one specific dataset!"
  }
}